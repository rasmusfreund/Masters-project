@article{bachPixelWiseExplanationsNonLinear2015,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
  editor = {Suarez, Oscar Deniz},
  date = {2015-07-10},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  url = {https://dx.plos.org/10.1371/journal.pone.0130140},
  urldate = {2024-04-28},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  langid = {english}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  date = {2001},
  journaltitle = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {08856125},
  doi = {10.1023/A:1010933404324},
  url = {http://link.springer.com/10.1023/A:1010933404324},
  urldate = {2024-05-15},
  file = {C:\Users\rasmu\Zotero\storage\PKW9Q3CL\A 1010933404324.pdf}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  date = {1995-09},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00994018},
  url = {http://link.springer.com/10.1007/BF00994018},
  urldate = {2024-05-15},
  abstract = {The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\HUNZFEMC\Cortes and Vapnik - 1995 - Support-vector networks.pdf}
}

@online{dhamdhereHowImportantNeuron2018,
  title = {How {{Important Is}} a {{Neuron}}?},
  author = {Dhamdhere, Kedar and Sundararajan, Mukund and Yan, Qiqi},
  date = {2018-05-30},
  eprint = {1805.12233},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.12233},
  urldate = {2024-04-28},
  abstract = {The problem of attributing a deep network’s prediction to its input/base features is well-studied (cf. [1]). We introduce the notion of conductance to extend the notion of attribution to the understanding the importance of hidden units.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{friedmanRegularizationPathsGeneralized2010,
	title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	date = {2010},
	journaltitle = {Journal of Statistical Software},
	shortjournal = {J. Stat. Soft.},
	volume = {33},
	number = {1},
	issn = {1548-7660},
	doi = {10.18637/jss.v033.i01},
	url = {http://www.jstatsoft.org/v33/i01/},
	urldate = {2024-05-18},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include 1 (the lasso), 2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	langid = {english},
	file = {C:\Users\rasmu\Zotero\storage\38B5BIMU\Friedman et al. - 2010 - Regularization Paths for Generalized Linear Models.pdf}
}


@article{ghahramaniProbabilisticMachineLearning2015,
  title = {Probabilistic Machine Learning and Artificial Intelligence},
  author = {Ghahramani, Zoubin},
  date = {2015-05-28},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {521},
  number = {7553},
  pages = {452--459},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14541},
  url = {https://www.nature.com/articles/nature14541},
  urldate = {2024-05-15},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\4YZHB4VG\Ghahramani - 2015 - Probabilistic machine learning and artificial inte.pdf}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  date = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-0-387-84858-7},
  url = {http://link.springer.com/10.1007/978-0-387-84858-7},
  urldate = {2024-05-15},
  isbn = {978-0-387-84857-0},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\UL52E552\Hastie et al. - 2009 - The Elements of Statistical Learning.pdf}
}

@article{hoerlRidgeRegressionBiased1970,
  title = {Ridge {{Regression}}: {{Biased Estimation}} for {{Nonorthogonal Problems}}},
  author = {Hoerl, Arthur E and Kennard, Robert W},
  date = {1970},
  journaltitle = {Technometrics},
  volume = {12},
  number = {1},
  pages = {55--67},
  doi = {10.2307/1267351},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\3H8DVSQ6\Hoerl and Kennard - Ridge Regression Biased Estimation for Nonorthogo.pdf}
}

@article{jordanMachineLearningTrends2015,
  title = {Machine Learning: {{Trends}}, Perspectives, and Prospects},
  shorttitle = {Machine Learning},
  author = {Jordan, M. I. and Mitchell, T. M.},
  date = {2015-07-17},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {349},
  number = {6245},
  pages = {255--260},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaa8415},
  url = {https://www.science.org/doi/10.1126/science.aaa8415},
  urldate = {2024-05-15},
  abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\BUJN94LN\Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf}
}

@article{kiranyaz1DConvolutionalNeural2021,
  title = {{{1D}} Convolutional Neural Networks and Applications: {{A}} Survey},
  shorttitle = {{{1D}} Convolutional Neural Networks and Applications},
  author = {Kiranyaz, Serkan and Avci, Onur and Abdeljaber, Osama and Ince, Turker and Gabbouj, Moncef and Inman, Daniel J.},
  date = {2021-04},
  journaltitle = {Mechanical Systems and Signal Processing},
  shortjournal = {Mechanical Systems and Signal Processing},
  volume = {151},
  pages = {107398},
  issn = {08883270},
  doi = {10.1016/j.ymssp.2020.107398},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0888327020307846},
  urldate = {2024-01-29},
  abstract = {During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feedforward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-theart performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap.},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\2RFC5C9F\1-s2.0-S0888327020307846-main.pdf}
}

@online{leeSurrogateLossFunction2021,
  title = {A Surrogate Loss Function for Optimization of \${{F}}\_\textbackslash beta\$ Score in Binary Classification with Imbalanced Data},
  author = {Lee, Namgil and Yang, Heejung and Yoo, Hojin},
  date = {2021-04-03},
  eprint = {2104.01459},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2104.01459},
  urldate = {2024-04-28},
  abstract = {The Fβ score is a commonly used measure of classification performance, which plays crucial roles in classification tasks with imbalanced data sets. However, the Fβ score cannot be used as a loss function by gradient-based learning algorithms for optimizing neural network parameters due to its non-differentiability. On the other hand, commonly used loss functions such as the binary cross-entropy (BCE) loss are not directly related to performance measures such as the Fβ score, so that neural networks optimized by using the loss functions may not yield optimal performance measures. In this study, we investigate a relationship between classification performance measures and loss functions in terms of the gradients with respect to the model parameters. Then, we propose a differentiable surrogate loss function for the optimization of the Fβ score. We show that the gradient paths of the proposed surrogate Fβ loss function approximate the gradient paths of the large sample limit of the Fβ score. Through numerical experiments using ResNets and benchmark image data sets, it is demonstrated that the proposed surrogate Fβ loss function is effective for optimizing Fβ scores under class imbalances in binary classification tasks compared with other loss functions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,I.5.2,Statistics - Machine Learning}
}

@online{leinoInfluenceDirectedExplanationsDeep2018,
  title = {Influence-{{Directed Explanations}} for {{Deep Convolutional Networks}}},
  author = {Leino, Klas and Sen, Shayak and Datta, Anupam and Fredrikson, Matt and Li, Linyi},
  date = {2018-11-13},
  eprint = {1802.03788},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.03788},
  urldate = {2024-04-28},
  abstract = {We study the problem of explaining a rich class of behavioral properties of deep neural networks. Distinctively, our influence-directed explanations approach this problem by peering inside the network to identify neurons with high influence on a quantity and distribution of interest, using an axiomaticallyjustified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by demonstrating a number of its unique capabilities on convolutional neural networks trained on ImageNet. Our evaluation demonstrates that influence-directed explanations (1) identify influential concepts that generalize across instances, (2) can be used to extract the “essence” of what the network learned about a class, and (3) isolate individual features the network uses to make decisions and distinguish related classes.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{marquardtRidgeRegressionPractice1975,
  title = {Ridge {{Regression}} in {{Practice}}},
  author = {Marquardt, Donald W and Snee, Ronald D},
  date = {1975},
  journaltitle = {The American Statistician},
  volume = {29},
  number = {1},
  pages = {3--20},
  doi = {10.1080/00031305.1975.10479105},
  abstract = {The use of biased estimation in data analysis and model building is discussed. A review of the theory of ridge regression and its relation to generalized inverse regression is presented along with the results of a simulation experiment and three examples of the use of ridge regression in practice. Comments on variable selection procedures, model validation, and ridge and generalized inverse regression computation procedures are included. The examples studied here show that when the predictor variables are highly correlated, ridge regression produces coefficients which predict and extrapolate better than least squares and is a safe procedure for selecting variables.},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\PXRCYK2U\Marquardt and Snee - 2024 - Ridge Regression in Practice.pdf}
}

@article{MonteCarloEvaluation2024,
  title = {A {{Monte Carlo Evaluation}} of {{Some Ridge-Type Estimators}}},
  date = {2024},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\RX5JQ2P5\2024 - A Monte Carlo Evaluation of Some Ridge-Type Estima.pdf}
}

@book{moodIntroductionTheoryStatistics1973,
	title = {Introduction to the Theory of Statistics},
	author = {Mood, Alexander McFarlane and Graybill, Franklin A. and Boes, Duane C.},
	date = {1973},
	series = {{{McGraw-Hill}} Series in Probability and Statistics},
	edition = {3d ed},
	publisher = {McGraw-Hill},
	location = {New York},
	isbn = {978-0-07-042864-5},
	pagetotal = {564},
	keywords = {Mathematical statistics},
	file = {C:\Users\rasmu\Zotero\storage\RMPFCG7I\intro_to_the_theory_of_statistics.pdf}
}

@article{murrayGlobalBurdenBacterial2022,
  title = {Global Burden of Bacterial Antimicrobial Resistance in 2019: A Systematic Analysis},
  shorttitle = {Global Burden of Bacterial Antimicrobial Resistance in 2019},
  author = {Murray, Christopher J L and Ikuta, Kevin Shunji and Sharara, Fablina and Swetschinski, Lucien and Robles Aguilar, Gisela and Gray, Authia and Han, Chieh and Bisignano, Catherine and Rao, Puja and Wool, Eve and Johnson, Sarah C and Browne, Annie J and Chipeta, Michael Give and Fell, Frederick and Hackett, Sean and Haines-Woodhouse, Georgina and Kashef Hamadani, Bahar H and Kumaran, Emmanuelle A P and McManigal, Barney and Achalapong, Sureeruk and Agarwal, Ramesh and Akech, Samuel and Albertson, Samuel and Amuasi, John and Andrews, Jason and Aravkin, Aleskandr and Ashley, Elizabeth and Babin, François-Xavier and Bailey, Freddie and Baker, Stephen and Basnyat, Buddha and Bekker, Adrie and Bender, Rose and Berkley, James A and Bethou, Adhisivam and Bielicki, Julia and Boonkasidecha, Suppawat and Bukosia, James and Carvalheiro, Cristina and Castañeda-Orjuela, Carlos and Chansamouth, Vilada and Chaurasia, Suman and Chiurchiù, Sara and Chowdhury, Fazle and Clotaire Donatien, Rafai and Cook, Aislinn J and Cooper, Ben and Cressey, Tim R and Criollo-Mora, Elia and Cunningham, Matthew and Darboe, Saffiatou and Day, Nicholas P J and De Luca, Maia and Dokova, Klara and Dramowski, Angela and Dunachie, Susanna J and Duong Bich, Thuy and Eckmanns, Tim and Eibach, Daniel and Emami, Amir and Feasey, Nicholas and Fisher-Pearson, Natasha and Forrest, Karen and Garcia, Coralith and Garrett, Denise and Gastmeier, Petra and Giref, Ababi Zergaw and Greer, Rachel Claire and Gupta, Vikas and Haller, Sebastian and Haselbeck, Andrea and Hay, Simon I and Holm, Marianne and Hopkins, Susan and Hsia, Yingfen and Iregbu, Kenneth C and Jacobs, Jan and Jarovsky, Daniel and Javanmardi, Fatemeh and Jenney, Adam W J and Khorana, Meera and Khusuwan, Suwimon and Kissoon, Niranjan and Kobeissi, Elsa and Kostyanev, Tomislav and Krapp, Fiorella and Krumkamp, Ralf and Kumar, Ajay and Kyu, Hmwe Hmwe and Lim, Cherry and Lim, Kruy and Limmathurotsakul, Direk and Loftus, Michael James and Lunn, Miles and Ma, Jianing and Manoharan, Anand and Marks, Florian and May, Jürgen and Mayxay, Mayfong and Mturi, Neema and Munera-Huertas, Tatiana and Musicha, Patrick and Musila, Lilian A and Mussi-Pinhata, Marisa Marcia and Naidu, Ravi Narayan and Nakamura, Tomoka and Nanavati, Ruchi and Nangia, Sushma and Newton, Paul and Ngoun, Chanpheaktra and Novotney, Amanda and Nwakanma, Davis and Obiero, Christina W and Ochoa, Theresa J and Olivas-Martinez, Antonio and Olliaro, Piero and Ooko, Ednah and Ortiz-Brizuela, Edgar and Ounchanum, Pradthana and Pak, Gideok D and Paredes, Jose Luis and Peleg, Anton Yariv and Perrone, Carlo and Phe, Thong and Phommasone, Koukeo and Plakkal, Nishad and Ponce-de-Leon, Alfredo and Raad, Mathieu and Ramdin, Tanusha and Rattanavong, Sayaphet and Riddell, Amy and Roberts, Tamalee and Robotham, Julie Victoria and Roca, Anna and Rosenthal, Victor Daniel and Rudd, Kristina E and Russell, Neal and Sader, Helio S and Saengchan, Weerawut and Schnall, Jesse and Scott, John Anthony Gerard and Seekaew, Samroeng and Sharland, Mike and Shivamallappa, Madhusudhan and Sifuentes-Osornio, Jose and Simpson, Andrew J and Steenkeste, Nicolas and Stewardson, Andrew James and Stoeva, Temenuga and Tasak, Nidanuch and Thaiprakong, Areerat and Thwaites, Guy and Tigoi, Caroline and Turner, Claudia and Turner, Paul and Van Doorn, H Rogier and Velaphi, Sithembiso and Vongpradith, Avina and Vongsouvath, Manivanh and Vu, Huong and Walsh, Timothy and Walson, Judd L and Waner, Seymour and Wangrangsimakul, Tri and Wannapinij, Prapass and Wozniak, Teresa and Young Sharma, Tracey E M W and Yu, Kalvin C and Zheng, Peng and Sartorius, Benn and Lopez, Alan D and Stergachis, Andy and Moore, Catrin and Dolecek, Christiane and Naghavi, Mohsen},
  date = {2022-02},
  journaltitle = {The Lancet},
  shortjournal = {The Lancet},
  volume = {399},
  number = {10325},
  pages = {629--655},
  issn = {01406736},
  doi = {10.1016/S0140-6736(21)02724-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673621027240},
  urldate = {2024-05-13},
  abstract = {Background Antimicrobial resistance (AMR) poses a major threat to human health around the world. Previous publications have estimated the effect of AMR on incidence, deaths, hospital length of stay, and health-care costs for specific pathogen–drug combinations in select locations. To our knowledge, this study presents the most comprehensive estimates of AMR burden to date.},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\U2SW5TRV\Murray et al. - 2022 - Global burden of bacterial antimicrobial resistanc.pdf}
}

@online{shrikumarLearningImportantFeatures2019,
  title = {Learning {{Important Features Through Propagating Activation Differences}}},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  date = {2019-10-12},
  eprint = {1704.02685},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1704.02685},
  urldate = {2024-04-28},
  abstract = {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its ‘reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{sohilIntroductionStatisticalLearning2022,
  title = {An Introduction to Statistical Learning with Applications in {{R}}: By {{Gareth James}}, {{Daniela Witten}}, {{Trevor Hastie}}, and {{Robert Tibshirani}}, {{New York}}, {{Springer Science}} and {{Business Media}}, 2013, \$41.98, {{eISBN}}: 978-1-4614-7137-7},
  shorttitle = {An Introduction to Statistical Learning with Applications in {{R}}},
  author = {Sohil, Fariha and Sohali, Muhammad Umair and Shabbir, Javid},
  date = {2022-01-02},
  journaltitle = {Statistical Theory and Related Fields},
  shortjournal = {Statistical Theory and Related Fields},
  volume = {6},
  number = {1},
  pages = {87--87},
  issn = {2475-4269, 2475-4277},
  doi = {10.1080/24754269.2021.1980261},
  url = {https://www.tandfonline.com/doi/full/10.1080/24754269.2021.1980261},
  urldate = {2024-05-16},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\5QB4XZY4\Sohil et al. - 2022 - An introduction to statistical learning with appli.pdf}
}

@book{suttonReinforcementLearningIntroduction,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S and Barto, Andrew G},
  date = {2018},
  edition = {2},
  publisher = {MIT press},
  location = {Cambridge, Massachusetts},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  isbn = {978-0-262-03924-6},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\F5XP4CZK\Sutton and Barto - Reinforcement Learning An Introduction.pdf}
}

@article{topolHighperformanceMedicineConvergence2019,
  title = {High-Performance Medicine: {{The}} Convergence of Human and Artificial Intelligence},
  shorttitle = {High-Performance Medicine},
  author = {Topol, Eric J.},
  date = {2019-01},
  journaltitle = {Nature Medicine},
  shortjournal = {Nat Med},
  volume = {25},
  number = {1},
  pages = {44--56},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-018-0300-7},
  url = {https://www.nature.com/articles/s41591-018-0300-7},
  urldate = {2024-05-15},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\4GGKHVJT\Topol - 2019 - High-performance medicine the convergence of huma.pdf}
}

@misc{weis2021driams,
  title = {Database of Resistance Information on Antimicrobials and {{MALDI-TOF}} Mass Spectra ({{DRIAMS}})},
  author = {Weis, Caroline V. and Cuénod, Aline and Rieck, Bastian and Dubuis, Olivier and Graf, Susanne and Lang, Claudia and Oberle, Michael and Brackmann, Maximilian and Søgaard, Kirstine K. and Osthoff, Michael and Borgwardt, Karsten and Egli, Adrian},
  date = {2021},
  doi = {10.5061/dryad.bzkh1899q},
  url = {https://datadryad.org/stash/dataset/doi:10.5061/dryad.bzkh1899q},
  organization = {Dryad}
}

@article{weisDirectAntimicrobialResistance2022,
  title = {Direct Antimicrobial Resistance Prediction from Clinical {{MALDI-TOF}} Mass Spectra Using Machine Learning},
  author = {Weis, Caroline and Cuénod, Aline and Rieck, Bastian and Dubuis, Olivier and Graf, Susanne and Lang, Claudia and Oberle, Michael and Brackmann, Maximilian and Søgaard, Kirstine K. and Osthoff, Michael and Borgwardt, Karsten and Egli, Adrian},
  date = {2022-01},
  journaltitle = {Nature Medicine},
  shortjournal = {Nat Med},
  volume = {28},
  number = {1},
  pages = {164--174},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-021-01619-9},
  url = {https://www.nature.com/articles/s41591-021-01619-9},
  urldate = {2023-12-11},
  langid = {english},
  file = {C:\Users\rasmu\Zotero\storage\49LYLQCQ\Weis-2022-Direct-antimicrobial-resistance-pre.pdf}
}

@article{tibshiraniRegressionShrinkageSelection1996,
	title = {Regression {{Shrinkage}} and {{Selection Via}} the {{Lasso}}},
	author = {Tibshirani, Robert},
	date = {1996-01-01},
	journaltitle = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	volume = {58},
	number = {1},
	pages = {267--288},
	issn = {1369-7412, 1467-9868},
	doi = {10.1111/j.2517-6161.1996.tb02080.x},
	url = {https://academic.oup.com/jrsssb/article/58/1/267/7027929},
	urldate = {2024-05-17},
	abstract = {We propose a new method for estimation in linear models. The 'lasso' minim residual sum of squares subject to the sum of the absolute value of the coefficients than a constant. Because of the nature of this constraint it tends to produce coefficients that are exactly 0 and hence gives interpretable models. Our simulatio suggest that the lasso enjoys some of the favourable properties of both subset sele ridge regression. It produces interpretable models like subset selection and exh stability of ridge regression. There is also an interesting relationship with recent adaptive function estimation by Donoho and Johnstone. The lasso idea is quite ge can be applied in a variety of statistical models: extensions to generalized regressio and tree-based models are briefly described.},
	langid = {english},
	file = {C:\Users\rasmu\Zotero\storage\J9TBZWVB\Tibshirani - 1996 - Regression Shrinkage and Selection Via the Lasso.pdf}
}

@book{whitlockAnalysisBiologicalData2015,
	title = {The Analysis of Biological Data},
	author = {Whitlock, Michael C. and Schluter, Dolph},
	date = {2015},
	edition = {Second edition},
	publisher = {Macmillan Education},
	location = {New York, NY},
	isbn = {978-1-319-15421-9},
	langid = {english},
	pagetotal = {818}
}

@article{zouRegularizationVariableSelection2005,
	title = {Regularization and {{Variable Selection Via}} the {{Elastic Net}}},
	author = {Zou, Hui and Hastie, Trevor},
	date = {2005-04-01},
	journaltitle = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	volume = {67},
	number = {2},
	pages = {301--320},
	issn = {1369-7412, 1467-9868},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	url = {https://academic.oup.com/jrsssb/article/67/2/301/7109482},
	urldate = {2024-05-17},
	abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
	langid = {english},
	file = {C:\Users\rasmu\Zotero\storage\7ADHVTEH\Zou and Hastie - 2005 - Regularization and Variable Selection Via the Elas.pdf}
}
