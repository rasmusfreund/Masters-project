\relax 
\abx@aux@refcontext{none/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{weis2021driams}
\abx@aux@segm{0}{0}{weis2021driams}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{jordanMachineLearningTrends2015}
\abx@aux@segm{0}{0}{jordanMachineLearningTrends2015}
\abx@aux@cite{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@segm{0}{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@cite{0}{suttonReinforcementLearningIntroduction}
\abx@aux@segm{0}{0}{suttonReinforcementLearningIntroduction}
\abx@aux@cite{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@segm{0}{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@segm{0}{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@segm{0}{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@cite{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@segm{0}{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@cite{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@segm{0}{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{zouRegularizationVariableSelection2005}
\abx@aux@segm{0}{0}{zouRegularizationVariableSelection2005}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ridge Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{8}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ridgelambdaeffect}{{1}{8}{Ridge Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces LASSO Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{9}{figure.caption.2}\protected@file@percent }
\newlabel{fig:lassolambdaeffect}{{2}{9}{LASSO Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.2}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Recursive Binary Splitting and Decision Tree Visualization.   This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The left plot (A) shows the decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. The right plot (B) visualizes the corresponding decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed.}}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:decisiontreevisualization}{{3}{11}{Recursive Binary Splitting and Decision Tree Visualization. \\ This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The left plot (A) shows the decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. The right plot (B) visualizes the corresponding decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed}{figure.caption.3}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{moodIntroductionTheoryStatistics1973}
\abx@aux@segm{0}{0}{moodIntroductionTheoryStatistics1973}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{cortesSupportvectorNetworks1995}
\abx@aux@segm{0}{0}{cortesSupportvectorNetworks1995}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Support Vector Machines: Linear and Non-linear Decision Boundaries.   \textbf  {Left}: SVM with a linear kernel creating a linear decision boundary. \textbf  {Right}: SVM with a radial basis function (RBF) kernel creating a non-linear decision boundary.}}{17}{figure.caption.4}\protected@file@percent }
\newlabel{fig:svm_examples}{{4}{17}{Support Vector Machines: Linear and Non-linear Decision Boundaries. \\ \textbf {Left}: SVM with a linear kernel creating a linear decision boundary. \textbf {Right}: SVM with a radial basis function (RBF) kernel creating a non-linear decision boundary}{figure.caption.4}{}}
\abx@aux@cite{0}{rosenblattPerceptronProbabilisticModel1958}
\abx@aux@segm{0}{0}{rosenblattPerceptronProbabilisticModel1958}
\abx@aux@cite{0}{hintonConnectionistLearningProcedures1989}
\abx@aux@segm{0}{0}{hintonConnectionistLearningProcedures1989}
\abx@aux@cite{0}{hornikMultilayerFeedforwardNetworks1989}
\abx@aux@segm{0}{0}{hornikMultilayerFeedforwardNetworks1989}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Perceptron algorithm process. The figure shows the progression of the decision boundary (dashed line) over multiple iterations of the algorithm.}}{19}{figure.caption.5}\protected@file@percent }
\newlabel{fig:perceptron_algorithm}{{5}{19}{Perceptron algorithm process. The figure shows the progression of the decision boundary (dashed line) over multiple iterations of the algorithm}{figure.caption.5}{}}
\abx@aux@cite{0}{hornikMultilayerFeedforwardNetworks1989}
\abx@aux@segm{0}{0}{hornikMultilayerFeedforwardNetworks1989}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of Perceptron and Multi-Layer Perceptron (MLP) on a non-linearly separable dataset. The top plots show the decision boundary progression for a Perceptron, while the bottom plots show the progression for an MLP.}}{20}{figure.caption.6}\protected@file@percent }
\newlabel{fig:perceptron_vs_mlp}{{6}{20}{Comparison of Perceptron and Multi-Layer Perceptron (MLP) on a non-linearly separable dataset. The top plots show the decision boundary progression for a Perceptron, while the bottom plots show the progression for an MLP}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Neural Network Structure with Variables. This figure illustrates a simple neural network with two input neurons (\( x_1 \) and \( x_2 \)), two hidden neurons (\( a_1 \) and \( a_2 \)), and one output neuron (\( y \)). The weights (\( w_{ij} \)) and biases (\( b_i \)) are labeled accordingly.}}{22}{figure.caption.7}\protected@file@percent }
\newlabel{fig:nn_feedforward}{{7}{22}{Neural Network Structure with Variables. This figure illustrates a simple neural network with two input neurons (\( x_1 \) and \( x_2 \)), two hidden neurons (\( a_1 \) and \( a_2 \)), and one output neuron (\( y \)). The weights (\( w_{ij} \)) and biases (\( b_i \)) are labeled accordingly}{figure.caption.7}{}}
\abx@aux@cite{0}{nielsenNeuralNetworksandDeepLearning2015}
\abx@aux@segm{0}{0}{nielsenNeuralNetworksandDeepLearning2015}
\newlabel{LastPage}{{}{32}{}{page.32}{}}
\gdef\lastpage@lastpage{32}
\gdef\lastpage@lastpageHy{32}
\abx@aux@read@bbl@mdfivesum{5728DB5628BAB31CB715E542AD6C69FE}
\abx@aux@defaultrefcontext{0}{murrayGlobalBurdenBacterial2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weis2021driams}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weisDirectAntimicrobialResistance2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jordanMachineLearningTrends2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ghahramaniProbabilisticMachineLearning2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{suttonReinforcementLearningIntroduction}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{friedmanRegularizationPathsGeneralized2010}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hastieElementsStatisticalLearning2009}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{whitlockAnalysisBiologicalData2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sohilIntroductionStatisticalLearning2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hoerlRidgeRegressionBiased1970}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{marquardtRidgeRegressionPractice1975}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tibshiraniRegressionShrinkageSelection1996}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zouRegularizationVariableSelection2005}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{moodIntroductionTheoryStatistics1973}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cortesSupportvectorNetworks1995}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblattPerceptronProbabilisticModel1958}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hintonConnectionistLearningProcedures1989}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hornikMultilayerFeedforwardNetworks1989}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nielsenNeuralNetworksandDeepLearning2015}{none/global//global/global/global}
\gdef \@abspage@last{34}
