\relax 
\abx@aux@refcontext{none/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{dauwalderMatrixAssistedLaser2023}
\abx@aux@segm{0}{0}{dauwalderMatrixAssistedLaser2023}
\abx@aux@cite{0}{weis2021driams}
\abx@aux@segm{0}{0}{weis2021driams}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{jordanMachineLearningTrends2015}
\abx@aux@segm{0}{0}{jordanMachineLearningTrends2015}
\abx@aux@cite{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@segm{0}{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@cite{0}{suttonReinforcementLearningIntroduction}
\abx@aux@segm{0}{0}{suttonReinforcementLearningIntroduction}
\abx@aux@cite{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@segm{0}{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@segm{0}{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@segm{0}{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@cite{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@segm{0}{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@cite{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@segm{0}{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{zouRegularizationVariableSelection2005}
\abx@aux@segm{0}{0}{zouRegularizationVariableSelection2005}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ridge Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{8}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ridgelambdaeffect}{{1}{8}{Ridge Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces LASSO Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{9}{figure.caption.2}\protected@file@percent }
\newlabel{fig:lassolambdaeffect}{{2}{9}{LASSO Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.2}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Recursive Binary Splitting and Decision Tree Visualization.   This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The top plot (A) visualizes the decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed. The bottom plot (B) shows the corresponding decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. }}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:decisiontreevisualization}{{3}{11}{Recursive Binary Splitting and Decision Tree Visualization. \\ This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The top plot (A) visualizes the decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed. The bottom plot (B) shows the corresponding decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines}{figure.caption.3}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{moodIntroductionTheoryStatistics1973}
\abx@aux@segm{0}{0}{moodIntroductionTheoryStatistics1973}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{cortesSupportvectorNetworks1995}
\abx@aux@segm{0}{0}{cortesSupportvectorNetworks1995}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Support Vector Machines: Linear and Non-linear Decision Boundaries.   \textbf  {Left}: SVM with a linear kernel creating a linear decision boundary. \textbf  {Right}: SVM with a radial basis function (RBF) kernel creating a non-linear decision boundary.}}{17}{figure.caption.4}\protected@file@percent }
\newlabel{fig:svm_examples}{{4}{17}{Support Vector Machines: Linear and Non-linear Decision Boundaries. \\ \textbf {Left}: SVM with a linear kernel creating a linear decision boundary. \textbf {Right}: SVM with a radial basis function (RBF) kernel creating a non-linear decision boundary}{figure.caption.4}{}}
\abx@aux@cite{0}{rosenblattPerceptronProbabilisticModel1958}
\abx@aux@segm{0}{0}{rosenblattPerceptronProbabilisticModel1958}
\abx@aux@cite{0}{hintonConnectionistLearningProcedures1989}
\abx@aux@segm{0}{0}{hintonConnectionistLearningProcedures1989}
\abx@aux@cite{0}{hornikMultilayerFeedforwardNetworks1989}
\abx@aux@segm{0}{0}{hornikMultilayerFeedforwardNetworks1989}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Perceptron algorithm process. The figure shows the progression of the decision boundary (solid line) over multiple iterations of the algorithm.}}{19}{figure.caption.5}\protected@file@percent }
\newlabel{fig:perceptron_algorithm}{{5}{19}{Perceptron algorithm process. The figure shows the progression of the decision boundary (solid line) over multiple iterations of the algorithm}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of perceptron and Multi-Layer Perceptron (MLP) on a non-linearly separable dataset. The top plots show the decision boundary progression for a Perceptron, while the bottom plots show the progression for an MLP.}}{20}{figure.caption.6}\protected@file@percent }
\newlabel{fig:perceptron_vs_mlp}{{6}{20}{Comparison of perceptron and Multi-Layer Perceptron (MLP) on a non-linearly separable dataset. The top plots show the decision boundary progression for a Perceptron, while the bottom plots show the progression for an MLP}{figure.caption.6}{}}
\abx@aux@cite{0}{hornikMultilayerFeedforwardNetworks1989}
\abx@aux@segm{0}{0}{hornikMultilayerFeedforwardNetworks1989}
\abx@aux@cite{0}{nielsenNeuralNetworksandDeepLearning2015}
\abx@aux@segm{0}{0}{nielsenNeuralNetworksandDeepLearning2015}
\abx@aux@cite{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@segm{0}{0}{osheaIntroductionConvolutionalNeural2015}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Illustration of feature extraction using convolutions. The input image (left) is convolved with a vertical edge detection kernel (middle) and a horizontal edge detection kernel (right). The resulting feature maps highlight vertical and horizontal edges, respectively.}}{24}{figure.caption.7}\protected@file@percent }
\newlabel{fig:nn_featureextraction}{{7}{24}{Illustration of feature extraction using convolutions. The input image (left) is convolved with a vertical edge detection kernel (middle) and a horizontal edge detection kernel (right). The resulting feature maps highlight vertical and horizontal edges, respectively}{figure.caption.7}{}}
\abx@aux@cite{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@segm{0}{0}{osheaIntroductionConvolutionalNeural2015}
\@writefile{toc}{\contentsline {paragraph}{Benefits of Sparse Interactions}{25}{paragraph*.8}\protected@file@percent }
\abx@aux@cite{0}{nirthikaPoolingConvolutionalNeural2022}
\abx@aux@segm{0}{0}{nirthikaPoolingConvolutionalNeural2022}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Visualization of a 2D convolution in a CNN. The input matrix \( \mathbf  {I} \) (left) is convolved with a 3x3 kernel \( \mathbf  {K} \) (center) to produce the output matrix \( \mathbf  {I * K} \) (right). The orange-highlighted region in \( \mathbf  {I} \) corresponds to the receptive field for the blue-highlighted element in \( \mathbf  {I * K} \). Dashed lines indicate the alignment and contribution of the kernel to the output value.}}{26}{figure.caption.9}\protected@file@percent }
\newlabel{fig:nn_receptivefield}{{8}{26}{Visualization of a 2D convolution in a CNN. The input matrix \( \mathbf {I} \) (left) is convolved with a 3x3 kernel \( \mathbf {K} \) (center) to produce the output matrix \( \mathbf {I * K} \) (right). The orange-highlighted region in \( \mathbf {I} \) corresponds to the receptive field for the blue-highlighted element in \( \mathbf {I * K} \). Dashed lines indicate the alignment and contribution of the kernel to the output value}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Pooling Methods}{27}{paragraph*.10}\protected@file@percent }
\abx@aux@cite{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@segm{0}{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@cite{0}{wrobelEfficientExplanationIndividual}
\abx@aux@segm{0}{0}{wrobelEfficientExplanationIndividual}
\abx@aux@cite{0}{selvarajuGradCAMVisualExplanations2020}
\abx@aux@segm{0}{0}{selvarajuGradCAMVisualExplanations2020}
\abx@aux@cite{0}{python}
\abx@aux@segm{0}{0}{python}
\abx@aux@cite{0}{weis2021driams}
\abx@aux@segm{0}{0}{weis2021driams}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Workflow of the analysis process, showing separate paths for species classification and resistance classification, including preprocessing, filtering, splitting, modeling, and evaluation steps.}}{30}{figure.caption.11}\protected@file@percent }
\newlabel{fig:workflow}{{9}{30}{Workflow of the analysis process, showing separate paths for species classification and resistance classification, including preprocessing, filtering, splitting, modeling, and evaluation steps}{figure.caption.11}{}}
\abx@aux@cite{0}{scikit-learn}
\abx@aux@segm{0}{0}{scikit-learn}
\abx@aux@cite{0}{2017arXiv170201460S}
\abx@aux@segm{0}{0}{2017arXiv170201460S}
\abx@aux@cite{0}{scikit-learn}
\abx@aux@segm{0}{0}{scikit-learn}
\abx@aux@cite{0}{NEURIPS2019_9015}
\abx@aux@segm{0}{0}{NEURIPS2019_9015}
\abx@aux@cite{0}{falcon2019pytorch}
\abx@aux@segm{0}{0}{falcon2019pytorch}
\abx@aux@cite{0}{wandb}
\abx@aux@segm{0}{0}{wandb}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces CNN Architecture}}{34}{figure.caption.13}\protected@file@percent }
\newlabel{fig:cnn_architecture}{{11}{34}{CNN Architecture}{figure.caption.13}{}}
\abx@aux@cite{0}{leeSurrogateLossFunction2021}
\abx@aux@segm{0}{0}{leeSurrogateLossFunction2021}
\abx@aux@cite{0}{kokhlikyanCaptumUnifiedGeneric}
\abx@aux@segm{0}{0}{kokhlikyanCaptumUnifiedGeneric}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Illustration of the preprocessing steps applied to an original MALDI-TOF mass spectrum. The top plot shows the original mass spectrum. The middle row includes three subplots displaying the intermediate steps: the log-transformed mass spectrum (left), the first LOWESS smoothing (middle), and the second LOWESS smoothing with normalization (right). The bottom plot presents the final rescaled and smoothed mass spectrum. This step-by-step preprocessing enhances the data quality by reducing noise and normalizing intensity values, facilitating more effective downstream analysis.}}{42}{figure.caption.12}\protected@file@percent }
\newlabel{fig:preprocessing_steps}{{10}{42}{Illustration of the preprocessing steps applied to an original MALDI-TOF mass spectrum. The top plot shows the original mass spectrum. The middle row includes three subplots displaying the intermediate steps: the log-transformed mass spectrum (left), the first LOWESS smoothing (middle), and the second LOWESS smoothing with normalization (right). The bottom plot presents the final rescaled and smoothed mass spectrum. This step-by-step preprocessing enhances the data quality by reducing noise and normalizing intensity values, facilitating more effective downstream analysis}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Distribution of Top 10 Species from 2015 to 2018. Each bar represents the percentage of the total species count for that year, with different colors indicating different species. The legend on the right shows the species names corresponding to each color. The "Remaining Species" category includes all species not within the top 10 for each respective year.}}{43}{figure.caption.14}\protected@file@percent }
\newlabel{fig:species_distribution}{{12}{43}{Distribution of Top 10 Species from 2015 to 2018. Each bar represents the percentage of the total species count for that year, with different colors indicating different species. The legend on the right shows the species names corresponding to each color. The "Remaining Species" category includes all species not within the top 10 for each respective year}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Principal Component Analysis (PCA) of the 2017 dataset by species. The top plot shows the distribution of samples based on the first two principal components (PC1 and PC2), while the bottom plot shows the distribution based on the third and fourth principal components (PC3 and PC4). Each point represents a sample, and the colors correspond to different species.}}{44}{figure.caption.15}\protected@file@percent }
\newlabel{fig:species_pca}{{13}{44}{Principal Component Analysis (PCA) of the 2017 dataset by species. The top plot shows the distribution of samples based on the first two principal components (PC1 and PC2), while the bottom plot shows the distribution based on the third and fourth principal components (PC3 and PC4). Each point represents a sample, and the colors correspond to different species}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Uniform Manifold Approximation and Projection (UMAP) of the 2017 dataset by species. Each point represents a sample, and the colors correspond to different species. The UMAP plot reveals distinct clusters for several species, indicating effective capture of dataset structure.}}{45}{figure.caption.16}\protected@file@percent }
\newlabel{fig:species_umap}{{14}{45}{Uniform Manifold Approximation and Projection (UMAP) of the 2017 dataset by species. Each point represents a sample, and the colors correspond to different species. The UMAP plot reveals distinct clusters for several species, indicating effective capture of dataset structure}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Comparison of species prediction performance across different machine learning models (Ridge, random forest (RF), Elastic Net, and Support Vector Machine (SVM)). Each point represents a five-fold cross-validation of a single hyper-parameter combination for the respective model and year. The boxplots summarize the distribution of these points for each model and year, including an aggregated category for all years combined. The y-axis shows the mean test score (accuracy).}}{45}{figure.caption.17}\protected@file@percent }
\newlabel{fig:model_comparison_species}{{15}{45}{Comparison of species prediction performance across different machine learning models (Ridge, random forest (RF), Elastic Net, and Support Vector Machine (SVM)). Each point represents a five-fold cross-validation of a single hyper-parameter combination for the respective model and year. The boxplots summarize the distribution of these points for each model and year, including an aggregated category for all years combined. The y-axis shows the mean test score (accuracy)}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Antibiotic resistance distribution for the year 2017. The stacked bar plot shows the log-transformed frequency of resistance for the top 25 antibiotics. Each bar is divided by species, with colors representing different species. The total resistance count for each antibiotic is shown in gray, with contributions from individual species stacked below. Within each bar, species are sorted by their frequency of resistance, with species contributing fewer samples placed lower in the bar. The plot highlights the predominant species contributing to antibiotic resistance.}}{46}{figure.caption.18}\protected@file@percent }
\newlabel{fig:antibiotic_resistance_distribution}{{16}{46}{Antibiotic resistance distribution for the year 2017. The stacked bar plot shows the log-transformed frequency of resistance for the top 25 antibiotics. Each bar is divided by species, with colors representing different species. The total resistance count for each antibiotic is shown in gray, with contributions from individual species stacked below. Within each bar, species are sorted by their frequency of resistance, with species contributing fewer samples placed lower in the bar. The plot highlights the predominant species contributing to antibiotic resistance}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Receiver Operating Charateristic (ROC) curves for a Ridge model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated).The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.78 to 0.87. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.78 to 0.95. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.8.}}{47}{figure.caption.19}\protected@file@percent }
\newlabel{fig:ROC_ridge}{{17}{47}{Receiver Operating Charateristic (ROC) curves for a Ridge model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated).The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.78 to 0.87. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.78 to 0.95. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.8}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Receiver Operating Characteristic (ROC) curves for a Random Forest model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated). The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.86 to 0.93. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.82 to 0.96. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.8.}}{48}{figure.caption.20}\protected@file@percent }
\newlabel{fig:ROC_rf}{{18}{48}{Receiver Operating Characteristic (ROC) curves for a Random Forest model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated). The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.86 to 0.93. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.82 to 0.96. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.8}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Receiver Operating Characteristic (ROC) curves for a Convolutional Neural Network (CNN) model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated). The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.85 to 0.94. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.85 to 0.98. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.85.}}{49}{figure.caption.21}\protected@file@percent }
\newlabel{fig:ROC_cnn}{{19}{49}{Receiver Operating Characteristic (ROC) curves for a Convolutional Neural Network (CNN) model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated). The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.85 to 0.94. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.85 to 0.98. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.85}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Feature importance values derived using Shapley value sampling for a CNN model predicting Tobramycin resistance. The top plot shows Shapley values across the mass spectrum, while the bottom plot overlays these values on the original MALDI-TOF mass spectrum. Positive and negative Shapley values indicate features that contribute positively and negatively to the prediction, respectively.}}{50}{figure.caption.22}\protected@file@percent }
\newlabel{fig:feature_shapley}{{20}{50}{Feature importance values derived using Shapley value sampling for a CNN model predicting Tobramycin resistance. The top plot shows Shapley values across the mass spectrum, while the bottom plot overlays these values on the original MALDI-TOF mass spectrum. Positive and negative Shapley values indicate features that contribute positively and negatively to the prediction, respectively}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Feature importance values derived using GradCAM for a CNN model predicting Tobramycin resistance. The top plot shows GradCAM values across the mass spectrum, while the bottom plot overlays these values on the original MALDI-TOF mass spectrum. GradCAM highlights regions of the spectrum that significantly influence the CNN's prediction.}}{50}{figure.caption.23}\protected@file@percent }
\newlabel{fig:feature_gradcam}{{21}{50}{Feature importance values derived using GradCAM for a CNN model predicting Tobramycin resistance. The top plot shows GradCAM values across the mass spectrum, while the bottom plot overlays these values on the original MALDI-TOF mass spectrum. GradCAM highlights regions of the spectrum that significantly influence the CNN's prediction}{figure.caption.23}{}}
\abx@aux@cite{0}{ottoStaphylococcusEpidermidisAccidental2009}
\abx@aux@segm{0}{0}{ottoStaphylococcusEpidermidisAccidental2009}
\abx@aux@cite{0}{russoMedicalEconomicImpact2003}
\abx@aux@segm{0}{0}{russoMedicalEconomicImpact2003}
\abx@aux@cite{0}{mortierBacterialSpeciesIdentification2021}
\abx@aux@segm{0}{0}{mortierBacterialSpeciesIdentification2021}
\abx@aux@cite{0}{tacconelliDiscoveryResearchDevelopment2018}
\abx@aux@segm{0}{0}{tacconelliDiscoveryResearchDevelopment2018}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{leeSurrogateLossFunction2021}
\abx@aux@segm{0}{0}{leeSurrogateLossFunction2021}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{wangRapidDetectionHeterogeneous2018}
\abx@aux@segm{0}{0}{wangRapidDetectionHeterogeneous2018}
\abx@aux@cite{0}{feucherollesCombinationMALDITOFMass2022}
\abx@aux@segm{0}{0}{feucherollesCombinationMALDITOFMass2022}
\abx@aux@cite{0}{singhalMALDITOFMassSpectrometry2015}
\abx@aux@segm{0}{0}{singhalMALDITOFMassSpectrometry2015}
\abx@aux@cite{0}{wangAllostericTransportMechanism2017}
\abx@aux@segm{0}{0}{wangAllostericTransportMechanism2017}
\abx@aux@cite{0}{jelschLactamaseTEM1Coli1992}
\abx@aux@segm{0}{0}{jelschLactamaseTEM1Coli1992}
\abx@aux@cite{0}{hobbsConservedSmallProtein2012}
\abx@aux@segm{0}{0}{hobbsConservedSmallProtein2012}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Zoomed-in feature importance values derived using GradCAM for Tobramycin resistance around 9085 m/z. The top plot shows the positive and negative feature importances, while the bottom plot overlays these values on the original MALDI-TOF mass spectrum. This detailed view highlights the significant impact of specific spectral features on the model's predictions.}}{58}{figure.caption.24}\protected@file@percent }
\newlabel{fig:tobramycin_zoomed}{{22}{58}{Zoomed-in feature importance values derived using GradCAM for Tobramycin resistance around 9085 m/z. The top plot shows the positive and negative feature importances, while the bottom plot overlays these values on the original MALDI-TOF mass spectrum. This detailed view highlights the significant impact of specific spectral features on the model's predictions}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Feature importance values derived using GradCAM for a CNN model predicting Clindamycin resistance. The top plot shows Shapley values across the mass spectrum, while the bottom plot overlays these values on the original MALDI-TOF mass spectrum.}}{59}{figure.caption.25}\protected@file@percent }
\newlabel{fig:clindamycin_gradcam}{{23}{59}{Feature importance values derived using GradCAM for a CNN model predicting Clindamycin resistance. The top plot shows Shapley values across the mass spectrum, while the bottom plot overlays these values on the original MALDI-TOF mass spectrum}{figure.caption.25}{}}
\abx@aux@cite{0}{dewaelePretrainedMaldiTransformers2024}
\abx@aux@segm{0}{0}{dewaelePretrainedMaldiTransformers2024}
\abx@aux@cite{0}{liuKANKolmogorovArnoldNetworks2024}
\abx@aux@segm{0}{0}{liuKANKolmogorovArnoldNetworks2024}
\abx@aux@cite{0}{sabourDynamicRoutingCapsules2017}
\abx@aux@segm{0}{0}{sabourDynamicRoutingCapsules2017}
\newlabel{RF1}{70}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces Distribution of Label Combinations between Train/Validation and Test Sets. The bar plot shows the frequency of some of the label combinations in the training/validation set (blue) and the test set (green). The x-axis represents the different label combinations, while the y-axis indicates the count of each combination in the respective datasets. This visualization demonstrates that the stratified split maintains the distribution of label combinations across the train/validation and test sets, ensuring that the datasets are representative of each other.}}{70}{figure.caption.26}\protected@file@percent }
\newlabel{fig:multilabel_split}{{S1}{70}{Distribution of Label Combinations between Train/Validation and Test Sets. The bar plot shows the frequency of some of the label combinations in the training/validation set (blue) and the test set (green). The x-axis represents the different label combinations, while the y-axis indicates the count of each combination in the respective datasets. This visualization demonstrates that the stratified split maintains the distribution of label combinations across the train/validation and test sets, ensuring that the datasets are representative of each other}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces Principal Components 1 and 2 from the 2017 dataset by species, including 'Remaining Species' which consists of all species outside the top 10.}}{71}{figure.caption.27}\protected@file@percent }
\newlabel{fig:pca_remaining}{{S2}{71}{Principal Components 1 and 2 from the 2017 dataset by species, including 'Remaining Species' which consists of all species outside the top 10}{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S3}{\ignorespaces UMAP of the 2017 dataset by species, including 'Remaining Species' which consists of all species outside the top 10.}}{71}{figure.caption.28}\protected@file@percent }
\newlabel{fig:umap_remaining}{{S3}{71}{UMAP of the 2017 dataset by species, including 'Remaining Species' which consists of all species outside the top 10}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S4}{\ignorespaces Principal Component Analysis (PCA) of the 2015 dataset by species.}}{72}{figure.caption.29}\protected@file@percent }
\newlabel{fig:pca_2015}{{S4}{72}{Principal Component Analysis (PCA) of the 2015 dataset by species}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S5}{\ignorespaces Principal Component Analysis (PCA) of the 2016 dataset by species.}}{73}{figure.caption.30}\protected@file@percent }
\newlabel{fig:pca_2016}{{S5}{73}{Principal Component Analysis (PCA) of the 2016 dataset by species}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S6}{\ignorespaces Principal Component Analysis (PCA) of the 2018 dataset by species.}}{74}{figure.caption.31}\protected@file@percent }
\newlabel{fig:pca_2018}{{S6}{74}{Principal Component Analysis (PCA) of the 2018 dataset by species}{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S7}{\ignorespaces UMAP of the 2015 dataset by species.}}{74}{figure.caption.32}\protected@file@percent }
\newlabel{fig:umap_2015}{{S7}{74}{UMAP of the 2015 dataset by species}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S8}{\ignorespaces UMAP of the 2017 dataset by species.}}{75}{figure.caption.33}\protected@file@percent }
\newlabel{fig:umap_2017}{{S8}{75}{UMAP of the 2017 dataset by species}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S9}{\ignorespaces UMAP of the 2018 dataset by species.}}{75}{figure.caption.34}\protected@file@percent }
\newlabel{fig:umap_2018}{{S9}{75}{UMAP of the 2018 dataset by species}{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S10}{\ignorespaces Antibiotic resistance distribution for 2015}}{76}{figure.caption.35}\protected@file@percent }
\newlabel{fig:resistance_distribution_2015}{{S10}{76}{Antibiotic resistance distribution for 2015}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S11}{\ignorespaces Antibiotic resistance distribution for 2016}}{76}{figure.caption.36}\protected@file@percent }
\newlabel{fig:resistance_distribution_2016}{{S11}{76}{Antibiotic resistance distribution for 2016}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S12}{\ignorespaces Antibiotic resistance distribution for 2018}}{77}{figure.caption.37}\protected@file@percent }
\newlabel{fig:resistance_distribution_2018}{{S12}{77}{Antibiotic resistance distribution for 2018}{figure.caption.37}{}}
\newlabel{LastPage}{{}{77}{}{page.77}{}}
\gdef\lastpage@lastpage{77}
\gdef\lastpage@lastpageHy{77}
\abx@aux@read@bbl@mdfivesum{8BA6EF49F20905CAFFBB00B81AFFC543}
\abx@aux@defaultrefcontext{0}{murrayGlobalBurdenBacterial2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dauwalderMatrixAssistedLaser2023}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weis2021driams}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weisDirectAntimicrobialResistance2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jordanMachineLearningTrends2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ghahramaniProbabilisticMachineLearning2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{suttonReinforcementLearningIntroduction}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{friedmanRegularizationPathsGeneralized2010}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hastieElementsStatisticalLearning2009}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{whitlockAnalysisBiologicalData2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sohilIntroductionStatisticalLearning2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hoerlRidgeRegressionBiased1970}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{marquardtRidgeRegressionPractice1975}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tibshiraniRegressionShrinkageSelection1996}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zouRegularizationVariableSelection2005}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{moodIntroductionTheoryStatistics1973}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cortesSupportvectorNetworks1995}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblattPerceptronProbabilisticModel1958}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hintonConnectionistLearningProcedures1989}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hornikMultilayerFeedforwardNetworks1989}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nielsenNeuralNetworksandDeepLearning2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{osheaIntroductionConvolutionalNeural2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nirthikaPoolingConvolutionalNeural2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krizhevskyImageNetClassificationDeep2017}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wrobelEfficientExplanationIndividual}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{selvarajuGradCAMVisualExplanations2020}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{python}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{scikit-learn}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{2017arXiv170201460S}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NEURIPS2019_9015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{falcon2019pytorch}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wandb}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{leeSurrogateLossFunction2021}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kokhlikyanCaptumUnifiedGeneric}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ottoStaphylococcusEpidermidisAccidental2009}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{russoMedicalEconomicImpact2003}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mortierBacterialSpeciesIdentification2021}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tacconelliDiscoveryResearchDevelopment2018}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wangRapidDetectionHeterogeneous2018}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{feucherollesCombinationMALDITOFMass2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{singhalMALDITOFMassSpectrometry2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wangAllostericTransportMechanism2017}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jelschLactamaseTEM1Coli1992}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hobbsConservedSmallProtein2012}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dewaelePretrainedMaldiTransformers2024}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liuKANKolmogorovArnoldNetworks2024}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sabourDynamicRoutingCapsules2017}{none/global//global/global/global}
\gdef \@abspage@last{79}
