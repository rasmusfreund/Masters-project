\relax 
\abx@aux@refcontext{none/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{weis2021driams}
\abx@aux@segm{0}{0}{weis2021driams}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{jordanMachineLearningTrends2015}
\abx@aux@segm{0}{0}{jordanMachineLearningTrends2015}
\abx@aux@cite{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@segm{0}{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@cite{0}{suttonReinforcementLearningIntroduction}
\abx@aux@segm{0}{0}{suttonReinforcementLearningIntroduction}
\abx@aux@cite{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@segm{0}{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@segm{0}{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@segm{0}{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@cite{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@segm{0}{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@cite{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@segm{0}{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{zouRegularizationVariableSelection2005}
\abx@aux@segm{0}{0}{zouRegularizationVariableSelection2005}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ridge Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{8}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ridgelambdaeffect}{{1}{8}{Ridge Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces LASSO Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{9}{figure.caption.2}\protected@file@percent }
\newlabel{fig:lassolambdaeffect}{{2}{9}{LASSO Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.2}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Recursive Binary Splitting and Decision Tree Visualization.   This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The left plot (A) shows the decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. The right plot (B) visualizes the corresponding decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed.}}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:decisiontreevisualization}{{3}{11}{Recursive Binary Splitting and Decision Tree Visualization. \\ This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The left plot (A) shows the decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. The right plot (B) visualizes the corresponding decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed}{figure.caption.3}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{moodIntroductionTheoryStatistics1973}
\abx@aux@segm{0}{0}{moodIntroductionTheoryStatistics1973}
\newlabel{LastPage}{{}{18}{}{page.18}{}}
\gdef\lastpage@lastpage{18}
\gdef\lastpage@lastpageHy{18}
\abx@aux@read@bbl@mdfivesum{AA9F5B648DBCDDBB5FAE7E3B8C3E7318}
\abx@aux@defaultrefcontext{0}{murrayGlobalBurdenBacterial2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weis2021driams}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weisDirectAntimicrobialResistance2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jordanMachineLearningTrends2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ghahramaniProbabilisticMachineLearning2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{suttonReinforcementLearningIntroduction}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{friedmanRegularizationPathsGeneralized2010}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hastieElementsStatisticalLearning2009}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{whitlockAnalysisBiologicalData2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sohilIntroductionStatisticalLearning2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hoerlRidgeRegressionBiased1970}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{marquardtRidgeRegressionPractice1975}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tibshiraniRegressionShrinkageSelection1996}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zouRegularizationVariableSelection2005}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{moodIntroductionTheoryStatistics1973}{none/global//global/global/global}
\gdef \@abspage@last{20}
