\relax 
\abx@aux@refcontext{none/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{weis2021driams}
\abx@aux@segm{0}{0}{weis2021driams}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{jordanMachineLearningTrends2015}
\abx@aux@segm{0}{0}{jordanMachineLearningTrends2015}
\abx@aux@cite{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@segm{0}{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@cite{0}{suttonReinforcementLearningIntroduction}
\abx@aux@segm{0}{0}{suttonReinforcementLearningIntroduction}
\abx@aux@cite{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@segm{0}{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@segm{0}{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@segm{0}{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@cite{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@segm{0}{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@cite{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@segm{0}{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{zouRegularizationVariableSelection2005}
\abx@aux@segm{0}{0}{zouRegularizationVariableSelection2005}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ridge Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{8}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ridgelambdaeffect}{{1}{8}{Ridge Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces LASSO Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{9}{figure.caption.2}\protected@file@percent }
\newlabel{fig:lassolambdaeffect}{{2}{9}{LASSO Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.2}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Recursive Binary Splitting and Decision Tree Visualization.   This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The left plot (A) shows the decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. The right plot (B) visualizes the corresponding decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed.}}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:decisiontreevisualization}{{3}{11}{Recursive Binary Splitting and Decision Tree Visualization. \\ This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The left plot (A) shows the decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. The right plot (B) visualizes the corresponding decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed}{figure.caption.3}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{moodIntroductionTheoryStatistics1973}
\abx@aux@segm{0}{0}{moodIntroductionTheoryStatistics1973}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{cortesSupportvectorNetworks1995}
\abx@aux@segm{0}{0}{cortesSupportvectorNetworks1995}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Support Vector Machines: Linear and Non-linear Decision Boundaries.   \textbf  {Left}: SVM with a linear kernel creating a linear decision boundary. \textbf  {Right}: SVM with a radial basis function (RBF) kernel creating a non-linear decision boundary.}}{17}{figure.caption.4}\protected@file@percent }
\newlabel{fig:svm_examples}{{4}{17}{Support Vector Machines: Linear and Non-linear Decision Boundaries. \\ \textbf {Left}: SVM with a linear kernel creating a linear decision boundary. \textbf {Right}: SVM with a radial basis function (RBF) kernel creating a non-linear decision boundary}{figure.caption.4}{}}
\abx@aux@cite{0}{rosenblattPerceptronProbabilisticModel1958}
\abx@aux@segm{0}{0}{rosenblattPerceptronProbabilisticModel1958}
\abx@aux@cite{0}{hintonConnectionistLearningProcedures1989}
\abx@aux@segm{0}{0}{hintonConnectionistLearningProcedures1989}
\abx@aux@cite{0}{hornikMultilayerFeedforwardNetworks1989}
\abx@aux@segm{0}{0}{hornikMultilayerFeedforwardNetworks1989}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Perceptron algorithm process. The figure shows the progression of the decision boundary (dashed line) over multiple iterations of the algorithm.}}{19}{figure.caption.5}\protected@file@percent }
\newlabel{fig:perceptron_algorithm}{{5}{19}{Perceptron algorithm process. The figure shows the progression of the decision boundary (dashed line) over multiple iterations of the algorithm}{figure.caption.5}{}}
\abx@aux@cite{0}{hornikMultilayerFeedforwardNetworks1989}
\abx@aux@segm{0}{0}{hornikMultilayerFeedforwardNetworks1989}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of Perceptron and Multi-Layer Perceptron (MLP) on a non-linearly separable dataset. The top plots show the decision boundary progression for a Perceptron, while the bottom plots show the progression for an MLP.}}{20}{figure.caption.6}\protected@file@percent }
\newlabel{fig:perceptron_vs_mlp}{{6}{20}{Comparison of Perceptron and Multi-Layer Perceptron (MLP) on a non-linearly separable dataset. The top plots show the decision boundary progression for a Perceptron, while the bottom plots show the progression for an MLP}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Neural Network Structure with Variables. This figure illustrates a simple neural network with two input neurons (\( x_1 \) and \( x_2 \)), two hidden neurons (\( a_1 \) and \( a_2 \)), and one output neuron (\( y \)). The weights (\( w_{ij} \)) and biases (\( b_i \)) are labeled accordingly.}}{22}{figure.caption.7}\protected@file@percent }
\newlabel{fig:nn_feedforward}{{7}{22}{Neural Network Structure with Variables. This figure illustrates a simple neural network with two input neurons (\( x_1 \) and \( x_2 \)), two hidden neurons (\( a_1 \) and \( a_2 \)), and one output neuron (\( y \)). The weights (\( w_{ij} \)) and biases (\( b_i \)) are labeled accordingly}{figure.caption.7}{}}
\abx@aux@cite{0}{nielsenNeuralNetworksandDeepLearning2015}
\abx@aux@segm{0}{0}{nielsenNeuralNetworksandDeepLearning2015}
\abx@aux@cite{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@segm{0}{0}{osheaIntroductionConvolutionalNeural2015}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Illustration of feature extraction using convolutions. The input image (left) is convolved with a vertical edge detection kernel (middle) and a horizontal edge detection kernel (right). The resulting feature maps highlight vertical and horizontal edges, respectively.}}{26}{figure.caption.8}\protected@file@percent }
\newlabel{fig:nn_featureextraction}{{8}{26}{Illustration of feature extraction using convolutions. The input image (left) is convolved with a vertical edge detection kernel (middle) and a horizontal edge detection kernel (right). The resulting feature maps highlight vertical and horizontal edges, respectively}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Benefits of Sparse Interactions}{27}{paragraph*.9}\protected@file@percent }
\abx@aux@cite{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@segm{0}{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@cite{0}{nirthikaPoolingConvolutionalNeural2022}
\abx@aux@segm{0}{0}{nirthikaPoolingConvolutionalNeural2022}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Illustration of the receptive field in a convolutional neural network. The input image is convolved with a 3x3 kernel (blue region) to produce a single value in the feature map (red region). This process demonstrates how each value in the feature map is influenced by a specific region of the input image, which is the receptive field of the neuron.}}{29}{figure.caption.10}\protected@file@percent }
\newlabel{fig:nn_receptivefield}{{9}{29}{Illustration of the receptive field in a convolutional neural network. The input image is convolved with a 3x3 kernel (blue region) to produce a single value in the feature map (red region). This process demonstrates how each value in the feature map is influenced by a specific region of the input image, which is the receptive field of the neuron}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Pooling Methods}{29}{paragraph*.11}\protected@file@percent }
\abx@aux@cite{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@segm{0}{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@cite{0}{krizhevskyImageNetClassificationDeep2017}
\abx@aux@segm{0}{0}{krizhevskyImageNetClassificationDeep2017}
\@writefile{toc}{\contentsline {paragraph}{Example:}{30}{paragraph*.12}\protected@file@percent }
\abx@aux@cite{0}{wrobelEfficientExplanationIndividual}
\abx@aux@segm{0}{0}{wrobelEfficientExplanationIndividual}
\abx@aux@cite{0}{selvarajuGradCAMVisualExplanations2020}
\abx@aux@segm{0}{0}{selvarajuGradCAMVisualExplanations2020}
\abx@aux@cite{0}{python}
\abx@aux@segm{0}{0}{python}
\abx@aux@cite{0}{weis2021driams}
\abx@aux@segm{0}{0}{weis2021driams}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{scikit-learn}
\abx@aux@segm{0}{0}{scikit-learn}
\abx@aux@cite{0}{2017arXiv170201460S}
\abx@aux@segm{0}{0}{2017arXiv170201460S}
\abx@aux@cite{0}{scikit-learn}
\abx@aux@segm{0}{0}{scikit-learn}
\abx@aux@cite{0}{NEURIPS2019_9015}
\abx@aux@segm{0}{0}{NEURIPS2019_9015}
\abx@aux@cite{0}{falcon2019pytorch}
\abx@aux@segm{0}{0}{falcon2019pytorch}
\abx@aux@cite{0}{wandb}
\abx@aux@segm{0}{0}{wandb}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Illustration of the preprocessing steps applied to an original MALDI-TOF MS spectrum. The top plot shows the original spectrum. The middle row includes three subplots displaying the intermediate steps: the log-transformed spectrum (left), the first LOWESS smoothing (middle), and the second LOWESS smoothing with normalization (right). The bottom plot presents the final rescaled and smoothed spectrum. This step-by-step preprocessing enhances the data quality by reducing noise and normalizing intensity values, facilitating more effective downstream analysis.}}{35}{figure.caption.13}\protected@file@percent }
\newlabel{fig:preprocessing_steps}{{10}{35}{Illustration of the preprocessing steps applied to an original MALDI-TOF MS spectrum. The top plot shows the original spectrum. The middle row includes three subplots displaying the intermediate steps: the log-transformed spectrum (left), the first LOWESS smoothing (middle), and the second LOWESS smoothing with normalization (right). The bottom plot presents the final rescaled and smoothed spectrum. This step-by-step preprocessing enhances the data quality by reducing noise and normalizing intensity values, facilitating more effective downstream analysis}{figure.caption.13}{}}
\abx@aux@cite{0}{leeSurrogateLossFunction2021}
\abx@aux@segm{0}{0}{leeSurrogateLossFunction2021}
\abx@aux@cite{0}{kokhlikyanCaptumUnifiedGeneric}
\abx@aux@segm{0}{0}{kokhlikyanCaptumUnifiedGeneric}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Distribution of Top 10 Species from 2015 to 2018. Each bar represents the percentage of the total species count for that year, with different colors indicating different species. The legend on the right shows the species names corresponding to each color. The "Remaining Species" category includes all species not within the top 10 for each respective year.}}{38}{figure.caption.14}\protected@file@percent }
\newlabel{fig:species_distribution}{{11}{38}{Distribution of Top 10 Species from 2015 to 2018. Each bar represents the percentage of the total species count for that year, with different colors indicating different species. The legend on the right shows the species names corresponding to each color. The "Remaining Species" category includes all species not within the top 10 for each respective year}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Principal Component Analysis (PCA) of the 2017 dataset by species. The top plot shows the distribution of samples based on the first two principal components (PC1 and PC2), while the bottom plot shows the distribution based on the third and fourth principal components (PC3 and PC4). Each point represents a sample, and the colors correspond to different species.}}{39}{figure.caption.15}\protected@file@percent }
\newlabel{fig:species_pca}{{12}{39}{Principal Component Analysis (PCA) of the 2017 dataset by species. The top plot shows the distribution of samples based on the first two principal components (PC1 and PC2), while the bottom plot shows the distribution based on the third and fourth principal components (PC3 and PC4). Each point represents a sample, and the colors correspond to different species}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Uniform Manifold Approximation and Projection (UMAP) of the 2017 dataset by species. Each point represents a sample, and the colors correspond to different species. The UMAP plot reveals distinct clusters for several species, indicating effective capture of dataset structure.}}{40}{figure.caption.16}\protected@file@percent }
\newlabel{fig:species_umap}{{13}{40}{Uniform Manifold Approximation and Projection (UMAP) of the 2017 dataset by species. Each point represents a sample, and the colors correspond to different species. The UMAP plot reveals distinct clusters for several species, indicating effective capture of dataset structure}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Comparison of species prediction performance across different machine learning models (Ridge, Random Forest (RF), Elastic Net, and Support Vector Machine (SVM)). Each point represents a five-fold cross-validation for one set of parameters for the respective model. The x-axis categorizes the data by year and includes an aggregated category for all years combined. The y-axis shows the mean test score (accuracy). SVM and Random Forest generally achieve the highest accuracy, with SVM outperforming all other models. Ridge Regression shows moderate performance, and Elastic Net consistently underperforms.}}{41}{figure.caption.17}\protected@file@percent }
\newlabel{fig:model_comparison_species}{{14}{41}{Comparison of species prediction performance across different machine learning models (Ridge, Random Forest (RF), Elastic Net, and Support Vector Machine (SVM)). Each point represents a five-fold cross-validation for one set of parameters for the respective model. The x-axis categorizes the data by year and includes an aggregated category for all years combined. The y-axis shows the mean test score (accuracy). SVM and Random Forest generally achieve the highest accuracy, with SVM outperforming all other models. Ridge Regression shows moderate performance, and Elastic Net consistently underperforms}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Antibiotic resistance distribution for the year 2017. The stacked bar plot shows the log-transformed frequency of resistance for the top 25 antibiotics. Each bar is divided by species, with colors representing different species. The total resistance count for each antibiotic is shown in gray, with contributions from individual species stacked below. Within each bar, species are sorted by their frequency of resistance, with species contributing fewer samples placed lower in the bar. The plot highlights the predominant species contributing to antibiotic resistance.}}{42}{figure.caption.18}\protected@file@percent }
\newlabel{fig:antibiotic_resistance_distribution}{{15}{42}{Antibiotic resistance distribution for the year 2017. The stacked bar plot shows the log-transformed frequency of resistance for the top 25 antibiotics. Each bar is divided by species, with colors representing different species. The total resistance count for each antibiotic is shown in gray, with contributions from individual species stacked below. Within each bar, species are sorted by their frequency of resistance, with species contributing fewer samples placed lower in the bar. The plot highlights the predominant species contributing to antibiotic resistance}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Receiver Operating Charateristic (ROC) curves for a Ridge model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated).The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.78 to 0.87. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.78 to 0.95. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.8.}}{43}{figure.caption.19}\protected@file@percent }
\newlabel{fig:ROC_ridge}{{16}{43}{Receiver Operating Charateristic (ROC) curves for a Ridge model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated).The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.78 to 0.87. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.78 to 0.95. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.8}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Receiver Operating Characteristic (ROC) curves for a Random Forest model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated). The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.86 to 0.93. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.82 to 0.96. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.8.}}{44}{figure.caption.20}\protected@file@percent }
\newlabel{fig:ROC_rf}{{17}{44}{Receiver Operating Characteristic (ROC) curves for a Random Forest model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated). The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.86 to 0.93. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.82 to 0.96. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.8}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Receiver Operating Characteristic (ROC) curves for a Convolutional Neural Network (CNN) model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated). The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.85 to 0.94. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.85 to 0.98. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.85.}}{46}{figure.caption.21}\protected@file@percent }
\newlabel{fig:ROC_cnn}{{18}{46}{Receiver Operating Characteristic (ROC) curves for a Convolutional Neural Network (CNN) model trained on the entire DRIAMS-A dataset (years 2015 to 2018 concatenated). The top plot displays the ROC curves for the first 15 antibiotics, with AUC values ranging from 0.85 to 0.94. The bottom plot shows the ROC curves for the remaining 12 antibiotics, with AUC values ranging from 0.85 to 0.98. The ROC curves illustrate the model's performance in classifying antibiotic resistance, with most antibiotics achieving AUC values above 0.85}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Feature importance values derived using Shapley Value Sampling for a CNN model predicting Tobramycin resistance. The top plot shows Shapley values across the m/z spectrum, while the bottom plot overlays these values on the original MALDI-TOF spectrum. Positive and negative Shapley values indicate features that contribute positively and negatively to the prediction, respectively.}}{47}{figure.caption.22}\protected@file@percent }
\newlabel{fig:feature_shapley}{{19}{47}{Feature importance values derived using Shapley Value Sampling for a CNN model predicting Tobramycin resistance. The top plot shows Shapley values across the m/z spectrum, while the bottom plot overlays these values on the original MALDI-TOF spectrum. Positive and negative Shapley values indicate features that contribute positively and negatively to the prediction, respectively}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Feature importance values derived using GradCAM for a CNN model predicting Tobramycin resistance. The top plot shows GradCAM values across the m/z spectrum, while the bottom plot overlays these values on the original MALDI-TOF spectrum. GradCAM highlights regions of the spectrum that significantly influence the CNN's prediction.}}{47}{figure.caption.23}\protected@file@percent }
\newlabel{fig:feature_gradcam}{{20}{47}{Feature importance values derived using GradCAM for a CNN model predicting Tobramycin resistance. The top plot shows GradCAM values across the m/z spectrum, while the bottom plot overlays these values on the original MALDI-TOF spectrum. GradCAM highlights regions of the spectrum that significantly influence the CNN's prediction}{figure.caption.23}{}}
\abx@aux@cite{0}{ottoStaphylococcusEpidermidisAccidental2009}
\abx@aux@segm{0}{0}{ottoStaphylococcusEpidermidisAccidental2009}
\abx@aux@cite{0}{russoMedicalEconomicImpact2003}
\abx@aux@segm{0}{0}{russoMedicalEconomicImpact2003}
\abx@aux@cite{0}{mortierBacterialSpeciesIdentification2021}
\abx@aux@segm{0}{0}{mortierBacterialSpeciesIdentification2021}
\abx@aux@cite{0}{tacconelliDiscoveryResearchDevelopment2018}
\abx@aux@segm{0}{0}{tacconelliDiscoveryResearchDevelopment2018}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{leeSurrogateLossFunction2021}
\abx@aux@segm{0}{0}{leeSurrogateLossFunction2021}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{wangRapidDetectionHeterogeneous2018}
\abx@aux@segm{0}{0}{wangRapidDetectionHeterogeneous2018}
\abx@aux@cite{0}{feucherollesCombinationMALDITOFMass2022}
\abx@aux@segm{0}{0}{feucherollesCombinationMALDITOFMass2022}
\abx@aux@cite{0}{singhalMALDITOFMassSpectrometry2015}
\abx@aux@segm{0}{0}{singhalMALDITOFMassSpectrometry2015}
\abx@aux@cite{0}{wangAllostericTransportMechanism2017}
\abx@aux@segm{0}{0}{wangAllostericTransportMechanism2017}
\abx@aux@cite{0}{jelschLactamaseTEM1Coli1992}
\abx@aux@segm{0}{0}{jelschLactamaseTEM1Coli1992}
\abx@aux@cite{0}{hobbsConservedSmallProtein2012}
\abx@aux@segm{0}{0}{hobbsConservedSmallProtein2012}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Zoomed-in feature importance values derived using GradCAM for Tobramycin resistance around 9085 m/z. The top plot shows the positive and negative feature importances, while the bottom plot overlays these values on the original MALDI-TOF spectrum. This detailed view highlights the significant impact of specific spectral features on the model's predictions.}}{55}{figure.caption.24}\protected@file@percent }
\newlabel{fig:tobramycin_zoomed}{{21}{55}{Zoomed-in feature importance values derived using GradCAM for Tobramycin resistance around 9085 m/z. The top plot shows the positive and negative feature importances, while the bottom plot overlays these values on the original MALDI-TOF spectrum. This detailed view highlights the significant impact of specific spectral features on the model's predictions}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Feature importance values derived using GradCAM for a CNN model predicting Clindamycin resistance. The top plot shows Shapley values across the m/z spectrum, while the bottom plot overlays these values on the original MALDI-TOF spectrum. This figure illustrates the variability in feature importance across different antibiotics, highlighting that not all classes are equally easy to determine.}}{57}{figure.caption.25}\protected@file@percent }
\newlabel{fig:clindamycin_gradcam}{{22}{57}{Feature importance values derived using GradCAM for a CNN model predicting Clindamycin resistance. The top plot shows Shapley values across the m/z spectrum, while the bottom plot overlays these values on the original MALDI-TOF spectrum. This figure illustrates the variability in feature importance across different antibiotics, highlighting that not all classes are equally easy to determine}{figure.caption.25}{}}
\newlabel{RF1}{65}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces Distribution of Label Combinations between Train/Validation and Test Sets. The bar plot shows the frequency of some of the label combinations in the training/validation set (blue) and the test set (green). The x-axis represents the different label combinations, while the y-axis indicates the count of each combination in the respective datasets. This visualization demonstrates that the stratified split maintains the distribution of label combinations across the train/validation and test sets, ensuring that the datasets are representative of each other.}}{65}{figure.caption.26}\protected@file@percent }
\newlabel{fig:multilabel_split}{{S1}{65}{Distribution of Label Combinations between Train/Validation and Test Sets. The bar plot shows the frequency of some of the label combinations in the training/validation set (blue) and the test set (green). The x-axis represents the different label combinations, while the y-axis indicates the count of each combination in the respective datasets. This visualization demonstrates that the stratified split maintains the distribution of label combinations across the train/validation and test sets, ensuring that the datasets are representative of each other}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces text}}{66}{figure.caption.27}\protected@file@percent }
\newlabel{fig:pca_remaining}{{S2}{66}{text}{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S3}{\ignorespaces text}}{66}{figure.caption.28}\protected@file@percent }
\newlabel{fig:umap_remaining}{{S3}{66}{text}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S4}{\ignorespaces Principal Component Analysis (PCA) of the 2015 dataset by species.}}{67}{figure.caption.29}\protected@file@percent }
\newlabel{fig:pca_2015}{{S4}{67}{Principal Component Analysis (PCA) of the 2015 dataset by species}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S5}{\ignorespaces Principal Component Analysis (PCA) of the 2016 dataset by species.}}{68}{figure.caption.30}\protected@file@percent }
\newlabel{fig:pca_2016}{{S5}{68}{Principal Component Analysis (PCA) of the 2016 dataset by species}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S6}{\ignorespaces Principal Component Analysis (PCA) of the 2018 dataset by species.}}{69}{figure.caption.31}\protected@file@percent }
\newlabel{fig:pca_2018}{{S6}{69}{Principal Component Analysis (PCA) of the 2018 dataset by species}{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S7}{\ignorespaces UMAP of the 2015 dataset by species.}}{69}{figure.caption.32}\protected@file@percent }
\newlabel{fig:umap_2015}{{S7}{69}{UMAP of the 2015 dataset by species}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S8}{\ignorespaces UMAP of the 2017 dataset by species.}}{70}{figure.caption.33}\protected@file@percent }
\newlabel{fig:umap_2017}{{S8}{70}{UMAP of the 2017 dataset by species}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S9}{\ignorespaces UMAP of the 2018 dataset by species.}}{70}{figure.caption.34}\protected@file@percent }
\newlabel{fig:umap_2018}{{S9}{70}{UMAP of the 2018 dataset by species}{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S10}{\ignorespaces Antibiotic resistance distribution for 2015}}{71}{figure.caption.35}\protected@file@percent }
\newlabel{fig:resistance_distribution_2015}{{S10}{71}{Antibiotic resistance distribution for 2015}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S11}{\ignorespaces Antibiotic resistance distribution for 2016}}{71}{figure.caption.36}\protected@file@percent }
\newlabel{fig:resistance_distribution_2016}{{S11}{71}{Antibiotic resistance distribution for 2016}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S12}{\ignorespaces Antibiotic resistance distribution for 2018}}{72}{figure.caption.37}\protected@file@percent }
\newlabel{fig:resistance_distribution_2018}{{S12}{72}{Antibiotic resistance distribution for 2018}{figure.caption.37}{}}
\newlabel{LastPage}{{}{72}{}{page.72}{}}
\gdef\lastpage@lastpage{72}
\gdef\lastpage@lastpageHy{72}
\abx@aux@read@bbl@mdfivesum{7D77591683729BA31E434210CE7FD107}
\abx@aux@defaultrefcontext{0}{murrayGlobalBurdenBacterial2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weis2021driams}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weisDirectAntimicrobialResistance2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jordanMachineLearningTrends2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ghahramaniProbabilisticMachineLearning2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{suttonReinforcementLearningIntroduction}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{friedmanRegularizationPathsGeneralized2010}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hastieElementsStatisticalLearning2009}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{whitlockAnalysisBiologicalData2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sohilIntroductionStatisticalLearning2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hoerlRidgeRegressionBiased1970}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{marquardtRidgeRegressionPractice1975}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tibshiraniRegressionShrinkageSelection1996}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zouRegularizationVariableSelection2005}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{moodIntroductionTheoryStatistics1973}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cortesSupportvectorNetworks1995}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblattPerceptronProbabilisticModel1958}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hintonConnectionistLearningProcedures1989}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hornikMultilayerFeedforwardNetworks1989}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nielsenNeuralNetworksandDeepLearning2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{osheaIntroductionConvolutionalNeural2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nirthikaPoolingConvolutionalNeural2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krizhevskyImageNetClassificationDeep2017}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wrobelEfficientExplanationIndividual}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{selvarajuGradCAMVisualExplanations2020}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{python}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{scikit-learn}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{2017arXiv170201460S}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NEURIPS2019_9015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{falcon2019pytorch}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wandb}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{leeSurrogateLossFunction2021}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kokhlikyanCaptumUnifiedGeneric}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ottoStaphylococcusEpidermidisAccidental2009}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{russoMedicalEconomicImpact2003}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mortierBacterialSpeciesIdentification2021}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tacconelliDiscoveryResearchDevelopment2018}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wangRapidDetectionHeterogeneous2018}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{feucherollesCombinationMALDITOFMass2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{singhalMALDITOFMassSpectrometry2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wangAllostericTransportMechanism2017}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jelschLactamaseTEM1Coli1992}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hobbsConservedSmallProtein2012}{none/global//global/global/global}
\gdef \@abspage@last{74}
