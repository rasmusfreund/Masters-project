\relax 
\abx@aux@refcontext{none/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@segm{0}{0}{murrayGlobalBurdenBacterial2022}
\abx@aux@cite{0}{weis2021driams}
\abx@aux@segm{0}{0}{weis2021driams}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{jordanMachineLearningTrends2015}
\abx@aux@segm{0}{0}{jordanMachineLearningTrends2015}
\abx@aux@cite{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@segm{0}{0}{ghahramaniProbabilisticMachineLearning2015}
\abx@aux@cite{0}{suttonReinforcementLearningIntroduction}
\abx@aux@segm{0}{0}{suttonReinforcementLearningIntroduction}
\abx@aux@cite{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@segm{0}{0}{friedmanRegularizationPathsGeneralized2010}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@segm{0}{0}{whitlockAnalysisBiologicalData2015}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@segm{0}{0}{hoerlRidgeRegressionBiased1970}
\abx@aux@cite{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@segm{0}{0}{marquardtRidgeRegressionPractice1975}
\abx@aux@cite{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@segm{0}{0}{tibshiraniRegressionShrinkageSelection1996}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{zouRegularizationVariableSelection2005}
\abx@aux@segm{0}{0}{zouRegularizationVariableSelection2005}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ridge Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{8}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ridgelambdaeffect}{{1}{8}{Ridge Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes the coefficients to shrink towards zero. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces LASSO Regression Coefficients and the Effect of Regularization.   The \textbf  {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf  {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}}{9}{figure.caption.2}\protected@file@percent }
\newlabel{fig:lassolambdaeffect}{{2}{9}{LASSO Regression Coefficients and the Effect of Regularization. \\ The \textbf {top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda $. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda $ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf {bottom plot} illustrates the effect of $\lambda $ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda $ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines}{figure.caption.2}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Recursive Binary Splitting and Decision Tree Visualization.   This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The left plot (A) shows the decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. The right plot (B) visualizes the corresponding decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed.}}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:decisiontreevisualization}{{3}{11}{Recursive Binary Splitting and Decision Tree Visualization. \\ This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The left plot (A) shows the decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. The right plot (B) visualizes the corresponding decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed}{figure.caption.3}{}}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{moodIntroductionTheoryStatistics1973}
\abx@aux@segm{0}{0}{moodIntroductionTheoryStatistics1973}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{cortesSupportvectorNetworks1995}
\abx@aux@segm{0}{0}{cortesSupportvectorNetworks1995}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@segm{0}{0}{sohilIntroductionStatisticalLearning2022}
\abx@aux@cite{0}{hastieElementsStatisticalLearning2009}
\abx@aux@segm{0}{0}{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Support Vector Machines: Linear and Non-linear Decision Boundaries.   \textbf  {Left}: SVM with a linear kernel creating a linear decision boundary. \textbf  {Right}: SVM with a radial basis function (RBF) kernel creating a non-linear decision boundary.}}{17}{figure.caption.4}\protected@file@percent }
\newlabel{fig:svm_examples}{{4}{17}{Support Vector Machines: Linear and Non-linear Decision Boundaries. \\ \textbf {Left}: SVM with a linear kernel creating a linear decision boundary. \textbf {Right}: SVM with a radial basis function (RBF) kernel creating a non-linear decision boundary}{figure.caption.4}{}}
\abx@aux@cite{0}{rosenblattPerceptronProbabilisticModel1958}
\abx@aux@segm{0}{0}{rosenblattPerceptronProbabilisticModel1958}
\abx@aux@cite{0}{hintonConnectionistLearningProcedures1989}
\abx@aux@segm{0}{0}{hintonConnectionistLearningProcedures1989}
\abx@aux@cite{0}{hornikMultilayerFeedforwardNetworks1989}
\abx@aux@segm{0}{0}{hornikMultilayerFeedforwardNetworks1989}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Perceptron algorithm process. The figure shows the progression of the decision boundary (dashed line) over multiple iterations of the algorithm.}}{19}{figure.caption.5}\protected@file@percent }
\newlabel{fig:perceptron_algorithm}{{5}{19}{Perceptron algorithm process. The figure shows the progression of the decision boundary (dashed line) over multiple iterations of the algorithm}{figure.caption.5}{}}
\abx@aux@cite{0}{hornikMultilayerFeedforwardNetworks1989}
\abx@aux@segm{0}{0}{hornikMultilayerFeedforwardNetworks1989}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of Perceptron and Multi-Layer Perceptron (MLP) on a non-linearly separable dataset. The top plots show the decision boundary progression for a Perceptron, while the bottom plots show the progression for an MLP.}}{20}{figure.caption.6}\protected@file@percent }
\newlabel{fig:perceptron_vs_mlp}{{6}{20}{Comparison of Perceptron and Multi-Layer Perceptron (MLP) on a non-linearly separable dataset. The top plots show the decision boundary progression for a Perceptron, while the bottom plots show the progression for an MLP}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Neural Network Structure with Variables. This figure illustrates a simple neural network with two input neurons (\( x_1 \) and \( x_2 \)), two hidden neurons (\( a_1 \) and \( a_2 \)), and one output neuron (\( y \)). The weights (\( w_{ij} \)) and biases (\( b_i \)) are labeled accordingly.}}{22}{figure.caption.7}\protected@file@percent }
\newlabel{fig:nn_feedforward}{{7}{22}{Neural Network Structure with Variables. This figure illustrates a simple neural network with two input neurons (\( x_1 \) and \( x_2 \)), two hidden neurons (\( a_1 \) and \( a_2 \)), and one output neuron (\( y \)). The weights (\( w_{ij} \)) and biases (\( b_i \)) are labeled accordingly}{figure.caption.7}{}}
\abx@aux@cite{0}{nielsenNeuralNetworksandDeepLearning2015}
\abx@aux@segm{0}{0}{nielsenNeuralNetworksandDeepLearning2015}
\abx@aux@cite{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@segm{0}{0}{osheaIntroductionConvolutionalNeural2015}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Illustration of feature extraction using convolutions. The input image (left) is convolved with a vertical edge detection kernel (middle) and a horizontal edge detection kernel (right). The resulting feature maps highlight vertical and horizontal edges, respectively.}}{26}{figure.caption.8}\protected@file@percent }
\newlabel{fig:nn_featureextraction}{{8}{26}{Illustration of feature extraction using convolutions. The input image (left) is convolved with a vertical edge detection kernel (middle) and a horizontal edge detection kernel (right). The resulting feature maps highlight vertical and horizontal edges, respectively}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Benefits of Sparse Interactions}{27}{paragraph*.9}\protected@file@percent }
\abx@aux@cite{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@segm{0}{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@cite{0}{nirthikaPoolingConvolutionalNeural2022}
\abx@aux@segm{0}{0}{nirthikaPoolingConvolutionalNeural2022}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Illustration of the receptive field in a convolutional neural network. The input image is convolved with a 3x3 kernel (blue region) to produce a single value in the feature map (red region). This process demonstrates how each value in the feature map is influenced by a specific region of the input image, which is the receptive field of the neuron.}}{29}{figure.caption.10}\protected@file@percent }
\newlabel{fig:nn_receptivefield}{{9}{29}{Illustration of the receptive field in a convolutional neural network. The input image is convolved with a 3x3 kernel (blue region) to produce a single value in the feature map (red region). This process demonstrates how each value in the feature map is influenced by a specific region of the input image, which is the receptive field of the neuron}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Pooling Methods}{29}{paragraph*.11}\protected@file@percent }
\abx@aux@cite{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@segm{0}{0}{osheaIntroductionConvolutionalNeural2015}
\abx@aux@cite{0}{krizhevskyImageNetClassificationDeep2017}
\abx@aux@segm{0}{0}{krizhevskyImageNetClassificationDeep2017}
\abx@aux@cite{0}{python}
\abx@aux@segm{0}{0}{python}
\@writefile{toc}{\contentsline {paragraph}{Example:}{30}{paragraph*.12}\protected@file@percent }
\abx@aux@cite{0}{weis2021driams}
\abx@aux@segm{0}{0}{weis2021driams}
\abx@aux@cite{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@segm{0}{0}{weisDirectAntimicrobialResistance2022}
\abx@aux@cite{0}{scikit-learn}
\abx@aux@segm{0}{0}{scikit-learn}
\abx@aux@cite{0}{2017arXiv170201460S}
\abx@aux@segm{0}{0}{2017arXiv170201460S}
\abx@aux@cite{0}{scikit-learn}
\abx@aux@segm{0}{0}{scikit-learn}
\abx@aux@cite{0}{NEURIPS2019_9015}
\abx@aux@segm{0}{0}{NEURIPS2019_9015}
\abx@aux@cite{0}{falcon2019pytorch}
\abx@aux@segm{0}{0}{falcon2019pytorch}
\abx@aux@cite{0}{wandb}
\abx@aux@segm{0}{0}{wandb}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Illustration of the preprocessing steps applied to an original MALDI-TOF MS spectrum. The top plot shows the original spectrum. The middle row includes three subplots displaying the intermediate steps: the log-transformed spectrum (left), the first LOWESS smoothing (middle), and the second LOWESS smoothing with normalization (right). The bottom plot presents the final rescaled and smoothed spectrum. This step-by-step preprocessing enhances the data quality by reducing noise and normalizing intensity values, facilitating more effective downstream analysis.}}{34}{figure.caption.13}\protected@file@percent }
\newlabel{fig:preprocessing_steps}{{10}{34}{Illustration of the preprocessing steps applied to an original MALDI-TOF MS spectrum. The top plot shows the original spectrum. The middle row includes three subplots displaying the intermediate steps: the log-transformed spectrum (left), the first LOWESS smoothing (middle), and the second LOWESS smoothing with normalization (right). The bottom plot presents the final rescaled and smoothed spectrum. This step-by-step preprocessing enhances the data quality by reducing noise and normalizing intensity values, facilitating more effective downstream analysis}{figure.caption.13}{}}
\newlabel{RF1}{42}
\@writefile{lof}{\contentsline {figure}{\numberline {Supplementary 1}{\ignorespaces Distribution of Label Combinations between Train/Validation and Test Sets. The bar plot shows the frequency of some of the label combinations in the training/validation set (blue) and the test set (green). The x-axis represents the different label combinations, while the y-axis indicates the count of each combination in the respective datasets. This visualization demonstrates that the stratified split maintains the distribution of label combinations across the train/validation and test sets, ensuring that the datasets are representative of each other.}}{42}{figure.caption.14}\protected@file@percent }
\newlabel{fig:multilabel_split}{{Supplementary 1}{42}{Distribution of Label Combinations between Train/Validation and Test Sets. The bar plot shows the frequency of some of the label combinations in the training/validation set (blue) and the test set (green). The x-axis represents the different label combinations, while the y-axis indicates the count of each combination in the respective datasets. This visualization demonstrates that the stratified split maintains the distribution of label combinations across the train/validation and test sets, ensuring that the datasets are representative of each other}{figure.caption.14}{}}
\newlabel{LastPage}{{}{42}{}{page.42}{}}
\gdef\lastpage@lastpage{42}
\gdef\lastpage@lastpageHy{42}
\abx@aux@read@bbl@mdfivesum{83D9E4D701B1BF2FE560FD6569E0D0D9}
\abx@aux@defaultrefcontext{0}{murrayGlobalBurdenBacterial2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weis2021driams}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weisDirectAntimicrobialResistance2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jordanMachineLearningTrends2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ghahramaniProbabilisticMachineLearning2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{suttonReinforcementLearningIntroduction}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{friedmanRegularizationPathsGeneralized2010}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hastieElementsStatisticalLearning2009}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{whitlockAnalysisBiologicalData2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sohilIntroductionStatisticalLearning2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hoerlRidgeRegressionBiased1970}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{marquardtRidgeRegressionPractice1975}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tibshiraniRegressionShrinkageSelection1996}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zouRegularizationVariableSelection2005}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{moodIntroductionTheoryStatistics1973}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cortesSupportvectorNetworks1995}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblattPerceptronProbabilisticModel1958}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hintonConnectionistLearningProcedures1989}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hornikMultilayerFeedforwardNetworks1989}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nielsenNeuralNetworksandDeepLearning2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{osheaIntroductionConvolutionalNeural2015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nirthikaPoolingConvolutionalNeural2022}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krizhevskyImageNetClassificationDeep2017}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{python}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{scikit-learn}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{2017arXiv170201460S}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{NEURIPS2019_9015}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{falcon2019pytorch}{none/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wandb}{none/global//global/global/global}
\gdef \@abspage@last{44}
