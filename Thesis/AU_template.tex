\documentclass[english,11pt,a4paper,titlepage]{article}

\usepackage{fontspec} % !!!Requires compiling with either XeLaTeX or LuaLaTeX!!! - allows for advanced font selection - also handles font encoding. If you wish to use PDFLaTeX with a natively supported font, you can use the following package instead:
% \usepackage[T1]{fontenc} 

\usepackage{amsmath} % Package for math notation
\usepackage{fontspec} % Requires either XeLaTeX or LuaLaTeX
\usepackage{titlesec} % Allows alternative section titles
\usepackage{lastpage} % Used for "n of m" page numbering
\usepackage{afterpage} % Used to remove background color from pages after title page
\usepackage{amsmath} % Package for math notation
\usepackage{amsfonts}
\usepackage{setspace}% Package for easy line spacing 
\usepackage{titlesec} % Allows alternative section titles
\usepackage{lastpage} % Used for "n of m" page numbering
\usepackage{afterpage} % Used to remove custom page color of the front page from the remaining pages
\usepackage{microtype} % Improves spacing between words and letters
\usepackage{abstract} % Custom abstract section
\usepackage{setspace} % Package for easy line spacing 
\usepackage{xcolor} % Kind of self-explanatory... Handles colors
\usepackage{graphicx} % Handling graphics
\usepackage{fancyhdr} % Increased control over headers and footers
\usepackage{caption}
\usepackage{rotating}

% ------Package suggestions------
\usepackage[backend=biber, style=numeric-comp, sorting=none]{biblatex} % Management of bibliography
% \usepackage{cleveref} % Enhanced cross-referencing of sections, figures, equations, etc.
\usepackage{booktabs} % For better looking tables - highly recommended!
% \usepackage{siunitx} % Useful for typesetting units and numbers
\usepackage{hyperref} % Enables hyperlinks within the LaTeX document and can make ToC, references, and more clickable
% \usepackage{listings} % Useful for writing algorithms / pseudocode
% \usepackage{mhchem} % Typesetting of chemical formulas and reactions
% ------------------------------

% Bibliography
\addbibresource{references.bib}

% Custom colours
\definecolor{sectioncolor}{RGB}{0,61,115} % Dark blue color 
\definecolor{subsectioncolor}{RGB}{75,75,74} % Grey color

% Heading font
\newfontfamily\headingfont{AUPassata_RG} % Standard AU font, replace this with your favourite font if needed

\titleformat*{\section}{\LARGE\headingfont\color{sectioncolor}}
\titleformat*{\subsection}{\Large\headingfont\color{subsectioncolor}}

% Configure the header
\pagestyle{fancy}
\fancyhf{} % Clears all header and footer fields
%\fancyhead[L]{\includegraphics[height=1.1cm]{img/au_blue2.png}} % No department
%\fancyhead[L]{\includegraphics[height=1.5cm]{img/inano.png}} % Interdisciplinary Nanoscience department
%\fancyhead[L]{\includegraphics[height=1.5cm]{img/mbg.png}} % Molecular Biology department
\fancyhead[L]{\includegraphics[height=1.5cm]{img/birc.png}} % Bioinformatics Research department

\fancyhead[R]{\includegraphics[height=1.5cm]{img/ausegl.png}} % Dolphin seal

\fancyfoot[C]{\headingfont\thepage} % Page number in footer
%\fancyfoot[C]{\headingfont\thepage\ of \pageref{LastPage}} % "m of n" page numbering in footer


\usepackage{geometry} % Allows easy change of page layout (margin size etc.)
% Page layout
\geometry{
	top=0.5cm, % Adjust top margin
	right=4.2cm, % Adjust right margin
	left=4.2cm, % Adjust left margin,
	bottom=3cm, % Adjust bottom margin
	headheight=3.5cm, % Adjust header height
	headsep=1.0cm, % Spacing between header and text
	includeheadfoot % Include header and footer
	}

% Main font
\setmainfont{Georgia}




\usepackage{kantlipsum} % Sample text - can be removed 




\newfontfamily\titlefont{AUPassata_BOLD} % Font for paper title

% ------------------------------ Custom Front Page Commands ------------------------------
% These commands define the layout and styling for the front pages of the document.
% They require 7 parameters:
% #1: The title of the document.
% #2: The first author's name.
% #3: The department or affiliation of the first author.
% #4: The second author's name (if applicable).
% #5: The department or affiliation of the second author (if applicable).
% #6: Path to an image file to be displayed on the front page.
% #7: Any additional information or text to be included at the bottom (e.g., supervisor's name).
% Example usage: \frontpageBlue{My Title}{My Name}{My Department}{Partner's Name}{Partner's Department}{path/to/image.jpg}{Additional Info}


% Setup for front page with blue background color
\newcommand{\frontpageBlue}[8]{
	\begin{titlepage}
		\pagecolor{sectioncolor}\afterpage{\nopagecolor}
		\centering
		\includegraphics[width=0.5\textwidth]{#7}\par
		\vspace{1cm}
		{\fontsize{34}{40}\selectfont\color{white}\titlefont #1\par} % Title
		\vspace{2cm}
		{\color{white}\Large\headingfont #2\par} % Author 1
		{\color{white}\large\headingfont #3\par} % Department author 1
		\vspace{1cm}
		{\color{white}\Large\headingfont #4\par} % Author 2
		{\color{white}\large\headingfont #5\par} % Department author 2
		\vspace{1cm}
		\includegraphics[width=0.5\textwidth]{#8} % Image
		\vfill
		%{\color{white}\Large\headingfont Supervisor\par} % Additional info
		{\color{white}\Large\headingfont #6}
	\end{titlepage}
}


% Setup for front page with white background color
\newcommand{\frontpageWhite}[8]{
	\begin{titlepage}
		\centering
		\includegraphics[width=0.5\textwidth]{#7}\par
		\vspace{1cm}
		{\fontsize{26}{28}\selectfont\color{sectioncolor}\titlefont #1\par} % Title
		\vspace{2cm}
		{\Large\headingfont\color{sectioncolor} #2\par} % Author 1
		{\large\headingfont\color{sectioncolor} #3\par} % Department author 1
		\vspace{0.1cm}
		{\Large\headingfont\color{sectioncolor} #4\par} % Author 2
		{\large\headingfont\color{sectioncolor} #5\par} % Department author 2
		\vspace{1cm}
		\includegraphics[width=0.5\textwidth]{#8} % Image
		\vfill
		%{\Large\headingfont\color{sectioncolor} Supervisor\par} % Additional information
		{\Large\headingfont\color{sectioncolor} #6}
	\end{titlepage}	
}

% Setting up the abstract section
\renewcommand{\abstractnamefont}{\normalfont\Large\bfseries} % Set the "Abstract" title to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to italic



%-----------------Beginning of document--------------------


\begin{document}
\setstretch{1.309} % Line spacing
\setlength{\parindent}{25pt} % Indentation at beginning of new paragraph


% ------ Blue front page ------
%\frontpageBlue{A systematic review of...}{Sample Author One}{Sample Department One}{Sample Author Two}{Sample Department Two}{}{img/ausegl_hvid.png}{img/au_white.png}


% ------ White front page ------
\frontpageWhite{Enhancing Antimicrobial Resistance Prediction with Convolutional Neural Networks}{Rasmus Freund}{Bioinformatics Research Center}{}{}{Supervisor\\Palle Villesen}{img/ausegl.png}{img/au_blue.png}


\begin{abstract}
	\noindent % No indentation for abstracts!
	\kant[1]
\end{abstract}

\captionsetup{font=small}

\section*{Introduction}
Antimicrobial resistance (AMR) presents one of the most daunting challenges in global public health, threatening to render ineffective the very drugs designed to protect us from bacterial infections. The pervasiveness and impact of AMR are profound, as bacteria continue to evolve mechanisms to survive against antibiotics, which have historically revolutionized the management of infectious diseases. 

In 2019, an estimated 4.95 million deaths were associated with bacterial AMR, with 1.27 million directly attributable to drug resistance	\cite{murrayGlobalBurdenBacterial2022}. The predominant pathogens contributing to AMR-related deaths include \textit{Eschericia coli}, \textit{Staphylococcus aureus}, and \textit{Klebsiella pneumoniae}, among others, which are responsible for a significant proportion of the mortality associated with drug-resistant infections. These bacteria are particularly dangerous due to their ability to resist multiple drugs, which complicates treatment options and increases the risk of severe outcomes \cite{murrayGlobalBurdenBacterial2022}.

Given the critical need for rapid and accurate antimicrobial resistance testing, this thesis explores advanced machine learning approaches to predict AMR directly from MALDI-TOF mass spectra of clinical isolates. Central to this work is the use of the Database of Resistance Information on Antimicrobials and MALDI-TOF Mass Spectra (DRIAMS), a comprehensive and publicly available dataset created by a collaborative effort of researchers from ETH ZÃ¼rich, the University of Basel, and other institutions \cite{weis2021driams}.

From 2016 to 2018, DRIAMS compiled over 300,000 mass spectra and more than 750,000 antimicrobial resistance phenotypes from clinical isolates collected across four diagnostic laboratories in Switzerland. This extensive dataset encompasses 803 different species of bacterial and fungal pathogens and is organized into four subcollections (DRIAMS-A to DRIAMS-D) \cite{weisDirectAntimicrobialResistance2022}. DRIAMS-A, the largest subcollection, serves as the primary focus of this thesis and contains 145,341 mass spectra linked to 71 different antimicrobial drugs.

To address the challenges posed by AMR, this thesis adopts a multi-stage approach, starting with the application of standard machine learning models to predict bacterial species from MALDI-TOF spectra. Building on this foundation, the focus then shifts to predicting AMR using a variety of machine learning techniques. As the complexity of the problem increases, more advanced techniques are employed to better capture the intricate patterns in the data and potentially enhance prediction accuracy and reliability. The following sections will explore these techniques in detail.


	\subsection*{Understanding Machine Learning}
	Machine learning (ML) is a subfield of artificial intelligence (AI) focused on developing algorithms that allow computers to learn from and make predictions or decisions based on data. Unlike traditional programming, where specific instructions are coded by humans, ML systems improve their performance on tasks through experience.
	
	The essence of ML lies in its ability to identify patterns and relationships with large datasets, which might be too complex or subtle for humans to discern. This capability is particularly valuable in fields like bioinformatics, where the volume and complexity of data, such as those found in genomic sequences or mass spectra, require advanced analytical methods.
	
	\subsubsection*{Basic Concepts of Machine Learning}
	At its core, ML can be divided into three main types: supervised learning, unsupervised learning, and reinforcement learning.
	\begin{itemize}
		\item \textbf{Supervised Learning}: In supervised learning, the algorithm is trained on a labeled dataset, meaning that each training example is paired with an output label. The goal is to learn a mapping from inputs to outputs that can be used to predict the labels of new, unseen examples. Common algorithms include linear regression, decision trees, and neural networks \cite{jordanMachineLearningTrends2015}.
		\item \textbf{Unsupervised Learning}: Unsupervised learning deals with unlabeled data. The algorithm tries to learn the underlying structure of the data without explicit instructions on what to predict. Techniques like clustering and dimensionality reduction fall under this category. Probabilistic models, such as Gaussian Mixture Models (GMM) and algorithms like k-means and with principal component analysis (PCA), are often used to uncover hidden patterns in the data \cite{ghahramaniProbabilisticMachineLearning2015}.
		\item \textbf{Reinforcement Learning}: In reinforcement learning, an agent learns to make decisions by performing actions in an environment to maximize cumulative reward. The agent, which can be a software program or a robot, interacts with the environment by taking actions and receiving feedback in the form of rewards or penalties. This feedback helps the agent learn the optimal strategy to achieve its goals over time \cite{suttonReinforcementLearningIntroduction}. While reinforcement learning represents a significant area of ML research, it is not utilized in this thesis. Its mention here serves to provide a comprehensive overview of the main types of machine learning.
	\end{itemize}
	
	\subsection*{Regularized Linear Regression: Ridge and LASSO}
	Regularized linear regression techniques, such Ridge and LASSO (short for Least Absolute Shrinkage and Selection Operator), address overfitting by introducing a penalty term to the least squares objective function. Overfitting occurs when a model is too complex and captures noise in the training data, leading to poor generalization on unseen data. These methods are particularly useful in high-dimensional scenarios, where they can improve generalization and perform feature selection.
	
	\subsubsection*{Mathematical Formulation}
	In standard linear regression, the objective is to minimize the sum of squared residuals:
	\begin{equation*} 
		\min_\beta \sum_{i=1}^{n}(y_i - X_i\beta)^2
	\end{equation*}
	where $X$ is the matrix of input features, $y$ is the vector of target values, and $\beta$ is the vector of coefficients.
	
	Ridge Regression modifies the objective by adding a regularization term that penalizes large coefficients:
	\begin{equation*}
		\min_\beta \left[\sum_{i=1}^{n}(y_i - X_i\beta)^2 + \lambda \sum_{j=1}^{p}\beta_{j}^{2}\right]
	\end{equation*}
	Here, $\sum_{j=1}^{p}\beta_{j}^{2}$ represents the L2 norm (Euclidean norm) of the coefficients, which is the sum of the squared values of the coefficients. The L2 norm penalizes large coefficients, encouraging them to be small but not necessarily zero.
	
	LASSO Regression, on the other hand, adds a regularization term that penalizes the absolute values of the coefficients:
	\begin{equation*}
		\min_\beta \left[\sum_{i=1}^{n}(y_i - X_i\beta)^2 + \lambda \sum_{j=1}^{p}|\beta_{j}|\right]
	\end{equation*}
	Here, $\sum_{j=1}^{p}|\beta_{j}|$ represents the L1 norm (Manhattan norm) of the coefficients, which is the sum of the absolute values of the coefficients. The L1 norm can drive some coefficients to exactly zero, performing feature selection.
	
	Additionally, we can see that setting \( \lambda = 0 \) for either Ridge or LASSO reduces the respective expression to ordinary least squares (OLS) regression.
	
	\subsubsection*{Application to Classification}
	While Ridge and LASSO are primarily used for regression tasks, they can be adapted for classification through logistic regression \cite{friedmanRegularizationPathsGeneralized2010,hastieElementsStatisticalLearning2009}. Logistic regression uses the logistic function to model the probability that a given input point belongs to a specific class:
	\begin{equation*}
		P(y=1|X) = \frac{1}{1 + e^{-X\beta}}
	\end{equation*}
	To fit a logistic regression model, we maximize the likelihood of the observed data. The likelihood function measures the probability of the observed labels given the input data and model parameters \cite{whitlockAnalysisBiologicalData2015}. For binary classification, the likelihood of the data is given by:
	\begin{equation*}
		L(\beta) = \prod_{i=1}^{n} P(y_i | X_i)
	\end{equation*}
	Taking the natural logarithm of the likelihood function, we obtain the log-likelihood:
	\begin{equation*}
		\log L(\beta) = \sum_{i=1}^{n} \left[ y_i \log P(y=1|X_i) + (1-y_i) \log (1 - P(y=1|X_i)) \right]
	\end{equation*}
	Maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood (NLL), which measures the discrepancy between the observed labels (\( y_i \)) and the predicted probabilities (\( P(y = 1 | X_i )\)):
	\begin{equation*}
		\text{NLL}(\beta) = -\sum_{i=1}^{n} \left[ y_i \log P(y=1|X_i) + (1-y_i) \log (1 - P(y=1|X_i)) \right]
	\end{equation*}
	To regularize logistic regression, the NLL is combined with a penalty term, as shown below.
	
	\noindent
	\textbf{Ridge Logistic Regression} \\
	Adds an L2 penalty term to the negative log-likelihood:
	\begin{equation*}
		\min_{\beta} \left[ -\sum_{i=1}^{n} \left( y_i \log P(y=1|X_i) + (1-y_i) \log (1 - P(y=1|X_i)) \right) + \lambda \sum_{j=1}^{p}\beta_{j}^{2} \right]
	\end{equation*}
	
	\noindent
	\textbf{LASSO Logistic Regression} \\
	Adds an L1 penalty term to the negative log-likelihood:
	\begin{equation*}
		\min_{\beta} \left[ -\sum_{i=1}^{n} \left( y_i \log P(y=1|X_i) + (1-y_i) \log (1 - P(y=1|X_i)) \right) + \lambda \sum_{j=1}^{p}|\beta_{j}| \right]
	\end{equation*}
	
	\subsubsection*{Properties and Benefits}
	\begin{itemize}
		\item \textbf{Bias-Variance Tradeoff}: Both Ridge and LASSO introduce bias by shrinking the coefficients, but this can reduce the model's variance and improve generalization performance \cite{sohilIntroductionStatisticalLearning2022} (see Figures \ref{fig:ridgelambdaeffect} and \ref{fig:lassolambdaeffect}).
		\item \textbf{Handling Multicollinearity (Ridge)}: Ridge Regression mitigates multicollinearity (high level of correlation between several input features) by shrinking coefficients, thus providing more stable estimates \cite{hoerlRidgeRegressionBiased1970,marquardtRidgeRegressionPractice1975}.
		\item \textbf{Feature Selection (LASSO)}: LASSO can shrink some coefficients to exactly zero, thus performing feature selection and simplifying the model. This is particularly beneficial in high-dimensional datasets \cite{tibshiraniRegressionShrinkageSelection1996}.
	\end{itemize}
	
	To effectively apply Ridge and LASSO regression, it is crucial to select the appropriate regularization parameter \( \lambda \) - cross-validation is a robust technique for determining an optimal value \cite{sohilIntroductionStatisticalLearning2022}. This process involves:
	\begin{enumerate}
		\item \textbf{Splitting the data}: Divide the dataset into \( k \) folds (e.g., \( k = 5 \) or \( k = 10 \))
		\item \textbf{Training and Validation}: Train the model on \( k - 1 \) folds and validate it on the remaining fold. This process is repeated \( k \) times, with each fold serving as the validation set once.
		\item \textbf{Averaging Performance}: Calculate the average performance metric (e.g., mean squared error) across all \( k \) folds for different values of \( \lambda \)
		\item \textbf{Selecting \( \lambda \)}: Choose the \( \lambda \) that minimizes the average validation error, ensuring the model generalizes well to unseen data
	\end{enumerate}
	
	\subsection*{Elastic Net Regression}
	Elastic Net Regression is a regularized regression technique that linearly combines the penalties of Ridge and LASSO. It is particularly useful when dealing with high-dimensional data where multicollinearity is present and when feature selection is necessary.
	
	\subsubsection*{Mathematical Formulation}
	Elastic Net modifies the objective of standard linear regression by adding two regularization terms:
	\begin{equation*}
		\min_\beta \left[\sum_{i=1}^{n}(y_i - X_i\beta)^2 + \lambda_1 \sum_{j=1}^{p}|\beta_{j}| + \lambda_2 \sum_{j=1}^{p}\beta_{j}^{2}\right]
	\end{equation*}
	Here, $\lambda_1$ controls the LASSO penalty (L1 norm) and $\lambda_2$ controls the Ridge penalty (L2 norm). When both $\lambda_1$ and $\lambda_2$ are zero, Elastic Net reduces to OLS regression. By adjusting these parameters, Elastic Net can balance between the benefits of Ridge and LASSO, shrinking some coefficients while performing feature selection \cite{zouRegularizationVariableSelection2005}.
	
	\begin{figure}[!t]
		\centering
		\includegraphics[width=1.0\linewidth]{img/ridge_lambda_effect}
		\caption{Ridge Regression Coefficients and the Effect of Regularization. \\ The \textbf{top plot} shows Ridge Regression coefficients as a function of the regularization parameter $\lambda$. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda$ causes the coefficients to shrink towards zero. The \textbf{bottom plot} illustrates the effect of $\lambda$ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda$ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}
		\label{fig:ridgelambdaeffect}
	\end{figure}
	
	\begin{figure}[!t]
		\centering
		\includegraphics[width=1.0\linewidth]{img/lasso_lambda_effect}
		\caption{LASSO Regression Coefficients and the Effect of Regularization. \\ The \textbf{top plot} shows LASSO Regression coefficients as a function of the regularization parameter $\lambda$. Each line represents a different feature's coefficient, demonstrating how increasing $\lambda$ causes some coefficients to shrink to zero, effectively performing feature selection. The \textbf{bottom plot} illustrates the effect of $\lambda$ on the fitted non-linear model for 10 data points (synthetic data). As $\lambda$ increases, the model transitions from overfitting (high variance) to better generalization (low variance), as seen by the smoothing of the fitted lines.}
		\label{fig:lassolambdaeffect}
	\end{figure}
	
	\clearpage
	
	\subsection*{Random Forest}
	Random Forest is an ensemble learning method used for both classification and regression tasks. It operates by constructing multiple decision trees during training, and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees. We will first explore how decision trees are created and move on to understanding the Random Forest method. Note that only the classification case will be explained, as this is the focus of the current work.
	
	\subsubsection*{Decision Trees}
	A decision tree is a flowchart-like structure in which each internal node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome. The path from the root to a leaf represents classification rules \cite{hastieElementsStatisticalLearning2009}. An example of a simple decision tree is shown in Figure \ref{fig:decisiontreevisualization}A; the Gini impurity value shown in the tree will be explained in the text below.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=1.0\linewidth]{img/decision_tree}
		\caption{Recursive Binary Splitting and Decision Tree Visualization. \\ This figure demonstrates the process of recursive binary splitting and the resulting decision tree on data that has two features: $X_1$ and $X_2$. The left plot (A) shows the decision boundaries created by a decision tree trained on a synthetic dataset, illustrating how the feature space is recursively split into regions, as indicated by the dashed lines. The right plot (B) visualizes the corresponding decision tree, with each internal node representing a decision rule based on feature thresholds, and leaf nodes representing class outcomes. Nodes are color-coded by the majority class, with class distributions and impurity measures (Gini index) displayed.}
		\label{fig:decisiontreevisualization}
	\end{figure}
	
	In constructing a decision tree, we aim to partition the feature space into regions where the response variable is as homogeneous as possible. This is done by recursively splitting the data based on certain criteria until a stopping condition is met.
	
	Given data that consists of \( p \) features and a response, for each of \( N \) observations \((x_i, y_i)\) for \( i = 1, 2, \ldots, N \), with \( x_i = (x_{i1}, x_{i2}, \ldots, x_{ip}) \), we model the response by partitioning the feature space into \( M \) regions \( R_1, R_2, \ldots, R_M \) and assigning class \( c_m \) to each region. The example in Figure \ref{fig:decisiontreevisualization} is split as follows:	
	\begin{enumerate}
		\item Feature 2 is split at \(X_2 = 0.26\) where the feature space at or below this point is assigned to class 0.
		\item Feature 1 is now split at \(X_1 = 1.459\).
		\item Finally, feature 1 is again split, now at \(X_1 = -1.043\).
	\end{enumerate}
	
	As visualized, this creates four regions in the feature space, which allows us to model the response as:
	\begin{equation*}
		\hat{f}(x) = \sum_{m=1}^{4} c_m I(x \in R_m)
	\end{equation*}
	where \( c_m \) is the class assigned to each region, and \( I(\cdots) \) is the indicator function, which is 1 if \( x \) is in region \( R_m \), and 0 otherwise. Since the data has only two classes, \( c_m \) will be either class 0 or class 1 for each region.
	
	But how do we decide where to split along each of the features? This is where we get into the splitting criterion, where there are two popular methods for classification tasks:
	
	\begin{itemize}
		\item \textbf{Gini impurity}:
		\begin{equation*}
			G = \sum_{i=1}^{C}p_i(1-p_i)
		\end{equation*}
		where $p_i$ is the probability of class $i$ in the node. This measures how often a randomly chosen element would be incorrectly classified.
		\item \textbf{Entropy}:
		\begin{equation*}
			H = -\sum_{i=1}^{C}p_i\log(p_i)
		\end{equation*}
		This measures the disorder or randomness of the data points in the node.
	\end{itemize}
	Since it is preferable to minimize both the Gini impurity and the entropy, the following step works for either criterion. Consider a feature \( j \) and a split point \( s \), and define the regions:
	\begin{equation*}
		R_1(j, s) = \{X|X_j \leq s\}\ \text{and}\ R_2(j, s) = \{X|X_j > s\}
	\end{equation*}
	
	The goal is then to find a \( j \) and \( s \) that minimize the function:
	\begin{equation*}
		\min_{j,s} \left[\frac{N_{R_1}}{N}C_{R_1} + \frac{N_{R_2}}{N}C_{R_2}\right]
	\end{equation*}
	where \( N_{R_1} \) and \( N_{R_2} \) are the number of samples in \( R_1 \) and \( R_2 \), and \( C_{R_1} \) and \( C_{R_2} \) is the splitting criterion value of these regions.
	
	After finding the best split, we now assign the class label for each region \( R_m \) as the majority class in that region:
	\begin{equation*}
		c_m = \arg \max_k \sum_{x_i \in R_m} I(y_i = k)
	\end{equation*}
	that is, the class label assigned for a given region is the class that appears most frequently within that region \cite{hastieElementsStatisticalLearning2009,sohilIntroductionStatisticalLearning2022}.
	
	Utilizing the functions mentioned so far for producing a decision tree could, however, lead to a problematic situation. One way to optimize the tree would be to grow it deep enough to assign each data point to its own node, resulting in an overly complex model. To prevent this, several methods can be employed:
	\begin{itemize}
		\item \textbf{Maximum depth}: Directly limit how deep the tree is allowed to grow.
		\item \textbf{Minimum samples per leaf}: Require a minimum number of samples in each leaf to avoid splits that result in nodes with very few data points.
		\item \textbf{Minimum samples per split}: Require a minimum number of samples to be present at a node before it can be split further.
		\item \textbf{Maximum leaf nodes}: Limit the number of leaf nodes in the tree to prevent excessive growth.
	\end{itemize}
	
	Once again, determining the optimal value for these hyperparameters is typically done through cross-validation.
	
	\subsubsection*{Random Forest}
	Building on the foundation of decision trees, Random Forest enhances the model's performance by combining multiple decision trees to form an ensemble. Each tree in the Random Forest is built using a bootstrap sample of the data, and at each split, a random subset of features is considered for splitting. The randomness helps in creating diverse trees that reduce the overall variance of the model.
	
	The Random Forest algorithm involves the following steps:
	\begin{enumerate}
		\item \textbf{Bootstrap sampling}: Randomly sample the dataset with replacement to create multiple bootstrap samples. Each tree is trained on a different bootstrap sample, introducing variability in the training data.
		\item \textbf{Tree construction}: For each bootstrap sample, grow a decision tree using a random subset of features at each split. This random selection of features further decorrelates the trees.
		\item \textbf{Aggregation}: Aggregate the predictions of all the trees to make the final prediction. For classification, the mode of the predicted classes is used.
	\end{enumerate}
	
	One might notice the aggressive attempt at decorrelating different trees in Random Forests. The reason for this is that if we can successfully create an ensemble of decorrelated trees, the variance of the model is reduced significantly more than if the trees are correlated. To understand this, consider \( n \) uncorrelated random variables \( X_1, X_2, \ldots, X_n \), each with the same variance \( \sigma^2 \). The variance of the sum of these variables is the sum of their variances \cite{moodIntroductionTheoryStatistics1973}. Mathematically:
	\begin{equation*}
		\text{Var}\left(\frac{1}{n} \sum_{i=1}^{n} X_i \right) = \frac{1}{n^2} \sum_{i=1}^{n} \text{Var}(X_i)
	\end{equation*}
	Simplifying:
	\begin{equation*}
		\frac{1}{n^2} \sum_{i=1}^{n} \text{Var}(X_i) = \frac{1}{n^2} \sum_{i=1}^{n} \sigma^2 = \frac{\sigma^2}{n}
	\end{equation*}
	Thus, the variance of the average of uncorrelated random variables decreases proportionally to \( \frac{1}{n} \).
	
	Now, if we instead consider \( n \) random variables \( X_1, X_2, \ldots, X_n \) that are highly correlated, let \( \rho \) represent the average correlation coefficient between any two variables.  For correlated variables, the covariance \( \text{Cov}(X_i, X_j) \) for \( i \neq j \) is \( \rho \sigma^2 \). 
	
	The variance now includes both the variance of the individual variables and the covariance between them:
	\begin{equation*}
		\text{Var}\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right) = \frac{1}{n^2} \left(\sum_{i=1}^{n} \text{Var}(X_i) + \sum_{i \neq j} \text{Cov}(X_i, X_j)\right)
	\end{equation*}
	Given that \( \text{Var}(X_i) = \sigma^2 \) and \( \text{Cov}(X_i, X_j) = \rho \sigma^2 \), we can simplify further:
	\begin{align*}
		\frac{1}{n^2} \left(\sum_{i=1}^{n} \text{Var}(X_i) + \sum_{i \neq j} \text{Cov}(X_i, X_j)\right) &= \frac{1}{n^2}(n\sigma^2 + n(n-1)\rho\sigma^2) \\
		&= \frac{\sigma^2}{n} + \rho\sigma^2\left(1 - \frac{1}{n}\right)
	\end{align*}
	
	This shows that the variance reduction achieved through averaging is much less significant when the variables are correlated, as the additional term \( \rho\sigma^2\left(1 - \frac{1}{n}\right) \) does not decrease as rapidly with increasing \( n \). Hence, the effectiveness of Random Forests in reducing variance is due to the decorrelation of trees, leading to a more robust and generalizable model.
	
	\subsection*{Support Vector Machines (SVMs)}
	Support Vector Machines (SVMs) are a powerful set of supervised learning methods used for classification, regression, and outlier detection. They are particularly effective in high-dimensional spaces and are versatile in terms of the different kernel functions that can be specified for the decision function.
	
	\subsubsection*{Mathematical Formulation}
	The primary goal of SVM is to find a hyperplane in an \( N \)-dimensional space (\( N \) being the number of features) that distinctly classifies the data points. The best hyperplane for an SVM means the one with the largest margin between the classes.
	
	\begin{equation*}
		\text{maximize} \quad M = \frac{2}{||\mathbf{w}||}
	\end{equation*}
	
	Subject to the constraints:
	\begin{equation*}
		y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 \quad \text{for all } i
	\end{equation*}
	
	Here, \( \mathbf{w} \) is the weight vector, \( \mathbf{x}_i \) are the feature vectors, \( y_i \) are the class labels, and \( b \) is the bias term. The margin \( M \) is defined as the distance between the hyperplane and the nearest data point from either class \cite{hastieElementsStatisticalLearning2009,cortesSupportvectorNetworks1995}. 
	
	\subsubsection*{Soft Margin SVM}
	In many real-world scenarios, data may not be perfectly linearly separable. To handle such cases, SVMs introduce slack variables \( \xi_i \) to allow some misclassifications:
	
	\begin{equation*}
		\text{minimize} \quad \frac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^{n} \xi_i
	\end{equation*}
	
	Subject to:
	\begin{equation*}
		y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1 - \xi_i \quad \text{and} \quad \xi_i \geq 0 \quad \text{for all } i
	\end{equation*}
	
	Here, \( C \) is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error \cite{hastieElementsStatisticalLearning2009,sohilIntroductionStatisticalLearning2022}.
	
	\subsubsection*{Kernel Trick}
	The power of SVMs lies in their ability to use kernel functions to handle non-linearly separable data by mapping the input features into high-dimensional feature spaces. Commonly used kernels include:
	
	\begin{itemize}
		\item \textbf{Linear kernel}: \( K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i \cdot \mathbf{x}_j \)
		\item \textbf{Polynomial kernel}: \( K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i \cdot \mathbf{x}_j + 1)^d \)
		\item \textbf{Radial Basis Function (RBF) kernel}: \( K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma ||\mathbf{x}_i - \mathbf{x}_j||^2) \)
		\item \textbf{Sigmoid kernel}: \( K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\alpha \mathbf{x}_i \cdot \mathbf{x}_j + c) \)
	\end{itemize}
	
	The kernel trick allows SVMs to perform linear separation in these higher-dimensional spaces without explicitly computing the coordinates of the data in that space \cite{sohilIntroductionStatisticalLearning2022,hastieElementsStatisticalLearning2009} (see Figure \ref{fig:svm_examples}).
	
	\subsubsection*{Properties and Benefits}
	\begin{itemize}
		\item \textbf{Effective in high-dimensional spaces}: SVMs are particularly effective when the number of dimensions exceeds the number of samples.
		\item \textbf{Memory efficiency}: Only a subset of training points (support vectors) are used in the decision function.
		\item \textbf{Versatility}: Different kernel functions can be specified for the decision function, making SVMs a versatile tool for various types of data.
	\end{itemize}
	
	\begin{figure}[th]
		\centering
		\includegraphics[width=0.45\linewidth]{img/svm_linear.png}
		\includegraphics[width=0.45\linewidth]{img/svm_nonlinear.png}
		\caption{Support Vector Machines: Linear and Non-linear Decision Boundaries. \\ \textbf{Left}: SVM with a linear kernel creating a linear decision boundary. \textbf{Right}: SVM with a radial basis function (RBF) kernel creating a non-linear decision boundary.}
		\label{fig:svm_examples}
	\end{figure}
	
	\subsubsection*{Hyperparameter Tuning}
	Choosing the right hyperparameters, such as the regularization parameter \( C \) and kernel-specific parameters like \( \gamma \) for the RBF kernel, is crucial for the performance of SVMs. Techniques such as cross-validation can be used to tune these parameters.
	
	\subsubsection*{Limitations}
	While SVMs are powerful, they have some limitations:
	\begin{itemize}
		\item \textbf{Computational complexity}: SVMs can be computationally intensive, especially with large datasets.
		\item \textbf{Choice of kernel}: The performance of SVMs depends significantly on the choice of the kernel and its parameters.
	\end{itemize}
	
	In conclusion, Support Vector Machines provide a robust method for classification tasks, especially in high-dimensional spaces. Their ability to handle non-linearly separable data through kernel functions makes them highly versatile and effective in diverse applications.
	
	
	\subsection*{Neural Networks}
	Neural Networks (NNs) are a class of machine learning models inspired by the human brain's structure and function. They consist of layers of interconnected neurons (nodes), each performing a simple computation. NNs are capable of learning complex patterns in data and have been successfully applied to a wide range of tasks, including image recognition, natural language processing, and more.
	
	\subsubsection*{Perceptron Algorithm}
	The Perceptron algorithm, developed by Frank Rosenblatt in 1957 \cite{rosenblattPerceptronProbabilisticModel1958}, is a fundamental building block of neural networks. It is a type of linear classifier, used for binary classification tasks. The Perceptron updates its weights iteratively to minimize classification errors.

	The Perceptron algorithm works as follows:
	\begin{enumerate}
		\item Initialize the weights to small random values.
		\item For each training example, compute the output:
		\begin{equation*}
			\hat{y} = \begin{cases}
				1 & \text{if } \mathbf{w} \cdot \mathbf{x} + b > 0 \\
				0 & \text{otherwise}
			\end{cases}
		\end{equation*}
		\item Update the weights if there is a misclassification:
		\begin{equation*}
			\mathbf{w} \leftarrow \mathbf{w} + \eta (y - \hat{y}) \mathbf{x}
		\end{equation*}
		\item Repeat steps 2 and 3 until convergence or for a fixed number of iterations.
	\end{enumerate}
	
	Here, \( \mathbf{w} \) represents the weight vector, \( \mathbf{x} \) is the input vector, \( b \) is the bias, \( y \) is the true label, \( \hat{y} \) is the predicted label, and \( \eta \) is the learning rate. Figure \ref{fig:perceptron_algorithm} shows a visualization of this process.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\linewidth]{img/perceptron.png}
		\caption{Perceptron algorithm process. The figure shows the progression of the decision boundary (dashed line) over multiple iterations of the algorithm.}
		\label{fig:perceptron_algorithm}
	\end{figure}
	
	\subsubsection*{Multi-Layer Perceptron (MLP)}
	The Multi-Layer Perceptron (MLP) is an extension of the Perceptron, consisting of multiple layers of neurons, including input, hidden, and output layers. Each layer in an MLP performs a linear transformation followed by a non-linear activation function, enabling the network to learn complex, non-linear patterns \cite{hintonConnectionistLearningProcedures1989}. Figure \ref{fig:perceptron_vs_mlp} shows a comparison between the Perceptron and MLP on a non-linearly separable dataset.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\linewidth]{img/perceptron_vs_mlp.png}
		\caption{Comparison of Perceptron and Multi-Layer Perceptron (MLP) on a non-linearly separable dataset. The top plots show the decision boundary progression for a Perceptron, while the bottom plots show the progression for an MLP.}
		\label{fig:perceptron_vs_mlp}
	\end{figure}
	
	\subsubsection*{Universal Approximation Theorem}
	The universal approximation theorem states that a feed-forward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of \(\mathbb{R}^n\), given sufficient number of neurons in the hidden layer \cite{hornikMultilayerFeedforwardNetworks1989}. This theorem deserves some attention to understand what it says, so let us break down the key parts:
	\begin{itemize}
		\item \textbf{Feed-forward neural network}: Neural network that takes some input data, passes it through its layers, and generates an output. The feed-forward process will be explained in further details below.
		\item \textbf{Compact subsets of \( \mathbb{R}^n \) }: This means that we are looking at functions defined in a specific, limited region of space (imagine a box in the \(n\)-dimensional space). The term "compact" ensures that this region is finite and closed, meaning it includes its boundary
	\end{itemize}
	To reiterate, the theorem tells us that with just one hidden layer and a sufficient number of neurons, a neural network can model highly complex behaviors and patterns in data. This underscores the theoretical power of neural networks to approximate any continuous function effectively:
	\begin{equation*}
		f(x) = \sum_{i=1}^{N} \alpha_i \sigma(\mathbf{w}_i \cdot \mathbf{x} + b_i)
	\end{equation*}
	where \( \sigma \) is an activation function, \( \mathbf{w}_i \) are the weights, \( \alpha_i \) and \( b_i \) are coefficients and biases \cite{hornikMultilayerFeedforwardNetworks1989}.
	
	\subsubsection*{Feed-Forward Process}
	The feed-forward process involves passing the input data through the layers of the network to generate an output. Each neuron computes a weighted sum of its inputs and applies an activation function:
	\begin{equation*}
		a_j = \sigma \left( \sum_{i=1}^{n} w_{ji} x_i + b_j \right)
	\end{equation*}
	where \( a_j \) is the activation of the neuron, \( w_{ji} \) are the weights, \( x_i \) are the inputs, \( b_j \) is the bias, and \( \sigma \) is the activation function. Common activation functions include the sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU), but many more exist.\\
	
	\textbf{Example:}
	Consider a simple neural network with one input layer, one hidden layer with two neurons, and one output neuron. Let's assume we are using the sigmoid activation function: \( \sigma(x) = (1 + \exp(-x))^{-1} \). See figure \ref{fig:nn_feedforward} for a visualization of this network.
	
	Given:
	\begin{itemize}
		\item Input features: \( x_1 \) and \( x_2 \)
		\item Weights for hidden neurons: \( w_{11}, w_{12}, w_{21}, w_{22} \)
		\item Bias for hidden neurons: \( b_1 \)
		\item Weights for output neuron: \( w_{31}, w_{32} \)
		\item Bias for output neuron: \( b_2 \)
	\end{itemize}
	
	1. Compute the activation for the hidden layer neurons:
	\begin{equation*}
		a_1 = \sigma (w_{11} x_1 + w_{12} x_2 + b_1)
	\end{equation*}
	\begin{equation*}
		a_2 = \sigma (w_{21} x_1 + w_{22} x_2 + b_1)
	\end{equation*}
	
	2. Compute the output of the network:
	\begin{equation*}
		y = \sigma (w_{31} a_1 + w_{32} a_2 + b_2)
	\end{equation*}
	
	This process shows how the input data is transformed layer by layer to produce the final output.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\linewidth]{img/nn_feedforward.png}
		\caption{Neural Network Structure with Variables. This figure illustrates a simple neural network with two input neurons (\( x_1 \) and \( x_2 \)), two hidden neurons (\( a_1 \) and \( a_2 \)), and one output neuron (\( y \)). The weights (\( w_{ij} \)) and biases (\( b_i \)) are labeled accordingly.}
		\label{fig:nn_feedforward}
	\end{figure}
	
	\subsubsection*{Backpropagation Algorithm}
	The backpropagation algorithm is used to train neural networks by minimizing the loss function. It involves two main steps: the forward pass and the backward pass. 
	
	\begin{enumerate}
		\item Forward Pass:
		\begin{itemize}
			\item Calculates activations and the output of the network.
		\end{itemize}
		\item Backward Pass:
		\begin{itemize}
			\item Calculates the gradients of the cost function with respect to each weight and bias, and updates the weights and biases.
		\end{itemize}
	\end{enumerate}
	
	The backpropagation process uses gradient descent to minimize the loss function. The weight update rule in gradient descent is given by:
	\begin{equation*}
		\Delta w_{ji} = -\eta \frac{\partial L}{\partial w_{ji}}
	\end{equation*}
	where \( \eta \) is the learning rate and \( \frac{\partial L}{\partial w_{ji}} \) is the partial derivative of the loss function \( L \) with respect to the weight \( w_{ji} \).
	
	To go through backpropagation, we make use of four fundamental equations \cite{nielsenNeuralNetworksandDeepLearning2015}. Before we go through these, we first define the intermediate quantity \( z^l \equiv w^l a^{l-1} + b^l \). This is to be understood as the weighted input of the neurons in layer \( l \), and is used extensively in the equations we're about to explain:
	
	1. Output error \(\delta^L\):
	\[
	\delta^L = \frac{\partial C}{\partial a^L} \cdot \sigma'(z^L)
	\]
    where \( \frac{\partial C}{\partial a^L} \) is the partial derivative of the cost function with respect to the activations in the output layer, and \( \sigma'(z^L) \) is the derivative of the activation function applied to the weighted input \( z^L \).
	
	2. Error backpropagation \(\delta^l\) for layer \( l \):
	\[
    \delta^l = \left( \sum_{k} \delta^{l+1}_k \cdot w_{k} \right) \cdot \sigma'(z^l)
	\]
    where \( \delta^{l+1}_k \) is the error term for neuron \( k \) in the next layer, and \( w_{k} \) is the weight connecting neuron \( k \) in the next layer to the current neuron.
	
	3. Gradient of the cost function with respect to biases:
	\[
	\frac{\partial C}{\partial b^l} = \delta^l
	\]
	
	4. Gradient of the cost function with respect to weights:
	\[
	\frac{\partial C}{\partial w^l} = a^{l-1} (\delta^l)^T
	\]
    where \( a^{l-1}_i \) is the activation of the previous layer neuron \( i \), and \( \delta^l_j \) is the error term for neuron \( j \) in the current layer.
    
	\textbf{Example:}
	Let's continue with the example from the feed-forward process. Assume our loss function \( L \) is the mean squared error between the predicted output \( y \) and the true label \( y_{true} \):
	
	\begin{equation*}
		L = \frac{1}{2}(y_{true} - y)^2
	\end{equation*}
	
	1. Compute the error at the output neuron:
	\begin{equation*}
		\delta^L = (y - y_{\text{true}}) \cdot \sigma'(z^L)
	\end{equation*}
	where \( z^L = w_{31} a_1 + w_{32} a_2 + b_2 \).
	
	2. Compute the errors at the hidden neurons:
	\begin{equation*}
		\delta_1 = \delta^L \cdot w_{31} \cdot \sigma'(z_1)
	\end{equation*}
	\begin{equation*}
		\delta_2 = \delta^L \cdot w_{32} \cdot \sigma'(z_2)
	\end{equation*}
	where \( z_1 = w_{11} x_1 + w_{21} x_2 + b_1 \) and \( z_2 = w_{12} x_1 + w_{22} x_2 + b_1 \).
	
	3. Update the weights and biases:
	\begin{equation*}
		w_{ji} \leftarrow w_{ji} - \eta \cdot \delta_j \cdot a_i
	\end{equation*}
	\begin{equation*}
		b_j \leftarrow b_j - \eta \cdot \delta_j
	\end{equation*}
	
	E.g., to update the weight \( w_{31} \) and the bias \( b_2 \):
	\begin{equation*}
		w_{31} \leftarrow w_{31} - \eta \cdot \delta^L \cdot a_1
	\end{equation*}
	\begin{equation*}
		b_2 \leftarrow b_2 - \eta \cdot \delta^L
	\end{equation*}
	
	By repeating these steps for many iterations, the neural network learns to minimize the loss function and accurately map inputs to outputs.
	
	
	\subsubsection*{Single vs. Multiple Output Neurons}
	Neural networks can be designed with different output structures depending on the task:
	\begin{itemize}
		\item \textbf{Single Output Neuron:} Used for binary classification tasks. Typically employs a sigmoid activation function at the output layer to produce probabilities.
		\begin{equation*}
			\sigma(z) = \frac{1}{1 + e^{-z}}
		\end{equation*}
		\item \textbf{Multiple Output Neurons:} Used for multi-class classification tasks. Employs the softmax activation function at the output layer to produce a probability distribution over classes.
		\begin{equation*}
			\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
		\end{equation*}
	\end{itemize}
	
	In summary, neural networks are a flexible and powerful model for learning complex patterns in data. Understanding the theoretical underpinnings such as the universal approximation theorem, and practical aspects like feed-forward processing and backpropagation, is essential for leveraging their full potential in various applications. However, traditional neural networks, such as Multi-Layer Perceptrons, face challenges when dealing with high-dimensional data, particularly images and spectral data, due to their lack of inherent spatial structure. Convolutional Neural Networks (CNNs), which we will dive into as the next topic, address these limitations by utilizing convolutional layers that can efficiently capture spatial hierarchies and patterns within the data, making them highly effective for image recognition, signal processing, and analysis of spectral data such as MALDI-TOF MS.
	
	\subsection*{Convolutional Neural Networks}
	It is important to note that while my work focuses on one-dimensional data from MALDI-TOF MS, this section will primarily use images (two-dimensional data) to explain the concepts of Convolutional Neural Networks (CNNs). The reason for this approach is that I believe images provide a more intuitive understanding of the subject due to their visual and spatial properties. The theory behind CNNs is directly applicable to one-dimensional data as well, allowing for a seamless transition of these concepts to MALDI-TOF MS data. Therefore, understanding CNNs in the context of image data will facilitate a clearer comprehension of their application to spectral and signal data.
	
	\subsubsection*{Feature Extraction with Convolutions}
	Convolutions are the fundamental building blocks of CNNs, allowing the network to extract meaningful features from the input data. By applying a set of filters (kernels) across the input, convolutions help identify patterns such as edges, textures, and more complex features in later layers \cite{osheaIntroductionConvolutionalNeural2015}.
	
	\textbf{Example:} Consider a simple 3x3 filter applied to an image. The filter slides over the image, performing element-wise multiplication and summing the results to produce a feature map. This process helps detect specific features, such as vertical or horizontal edges. This effect is shown in Figure \ref{fig:nn_featureextraction} where the input image (left) is convolved with two different kernels that detect vertical edges (middle) and horizontal edges (right).
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.9\linewidth]{img/kernels.png}
		\caption{Illustration of feature extraction using convolutions. The input image (left) is convolved with a vertical edge detection kernel (middle) and a horizontal edge detection kernel (right). The resulting feature maps highlight vertical and horizontal edges, respectively.}
		\label{fig:nn_featureextraction}
	\end{figure}
	
	\subsubsection*{Sparse Interactions}
	In a CNN, each neuron in a layer is connected to a small region of the input known as the receptive field, rather than the entire input. This sparse connectivity ensures that the network focuses on local patterns, making the model more efficient and reducing the number of parameters compared to fully connected layers.
	
	Sparse interactions mean that a single convolutional filter (kernel) is applied locally, which allows the network to learn spatial hierarchies. For instance, in an image, lower layers might learn to detect simple features such as edges or textures, while higher layers might combine these features to detect more complex patterns like shapes or objects. By limiting the connections to a local region, CNNs can efficiently capture spatial dependencies.
	
	\paragraph{Benefits of Sparse Interactions}
	\begin{itemize}
		\item \textbf{Efficiency}: Sparse interactions significantly reduce the number of parameters in the model. For a typical fully connected layer with \(n\) input units and \(m\) output units, there are \(n \times m\) parameters. In contrast, a convolutional layer with a filter size of \(k \times k\) applied to an \(n \times n\) input has \(k^2\) parameters per filter. Since CNNs typically use multiple filters, the number of parameters is \( k^2 \times f \), where \( f \) is the number of filters. This is still much smaller and depends only on the filter size and the number of filters, not the input size.
		\item \textbf{Parameter Sharing}: The same filter (set of parameters) is applied across different regions of the input, which allows the network to detect the same feature in different parts of the input. This parameter sharing leads to better generalization and reduces the risk of overfitting.
		\item \textbf{Local Receptive Fields}: Each neuron only focuses on a small region of the input, which enables the network to learn local features effectively. This local focus is crucial for tasks like image recognition, where spatial hierarchies are essential.
	\end{itemize}
	
	\textbf{Example:} Consider an input image of size 32x32 pixels. A convolutional layer with a 5x5 filter scans across the image, creating a feature map. Each neuron in this feature map is connected to a 5x5 region of the input image, resulting in a sparse interaction. If the filter moves one pixel at a time (stride of 1), the feature map will have a size of 28x28 (since the filter cannot go beyond the boundaries of the image without padding). This localized interaction helps the network learn relevant features while keeping the number of parameters low.\\
	
	By leveraging sparse interactions, CNNs are able to efficiently process high-dimensional data such as images, learning hierarchical features that are essential for complex pattern recognition tasks.
	
	\subsubsection*{Receptive Field}
	The receptive field refers to the region of the input that a particular neuron is responsive to. In a convolutional neural network (CNN), neurons in a layer are connected only to a small, localized region of the input, known as the receptive field. As the network goes deeper, the receptive field increases, allowing the network to capture more complex and abstract features \cite{osheaIntroductionConvolutionalNeural2015}.
	
	For example, in the first convolutional layer, each neuron might be connected to a 3x3 region of the input image (as shown in Figure \ref{fig:nn_receptivefield}). As we add more convolutional layers, the receptive field of neurons in higher layers becomes larger. This means that each neuron in these deeper layers is influenced by a larger portion of the original input image. Consequently, the network can learn hierarchical features, starting with simple edges and textures in the early layers and moving to more complex patterns and objects in the deeper layers.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.9\linewidth]{img/receptive_field.png}
		\caption{Illustration of the receptive field in a convolutional neural network. The input image is convolved with a 3x3 kernel (blue region) to produce a single value in the feature map (red region). This process demonstrates how each value in the feature map is influenced by a specific region of the input image, which is the receptive field of the neuron.}
		\label{fig:nn_receptivefield}
	\end{figure}
	
	\subsubsection*{Pooling Layers}
	Pooling layers are a critical component of convolutional neural networks (CNNs). They perform downsampling operations that reduce the spatial dimensions (width and height) of the input feature maps while retaining important features \cite{nirthikaPoolingConvolutionalNeural2022}. Pooling serves several key purposes in CNNs:
	\begin{itemize}
		\item \textbf{Dimensionality Reduction:} By reducing the spatial dimensions of the feature maps, pooling layers decrease the number of parameters and computational complexity of the model. This makes the network more efficient and faster to train.
		\item \textbf{Translation Invariance:} Pooling helps the network to become more robust to translations and distortions in the input data. By summarizing local regions, it ensures that small shifts or changes in the input do not significantly affect the output.
		\item \textbf{Overfitting Reduction:} By reducing the number of parameters, pooling layers help to prevent overfitting, especially when working with smaller datasets.
	\end{itemize}
	
	\paragraph{Common Pooling Methods}
	Several pooling methods are commonly used in CNNs:
	\begin{itemize}
		\item \textbf{Max Pooling:} Max pooling selects the maximum value within a local patch of the feature map. It effectively retains the most prominent features in the local region.
		\begin{equation*}
			y_{i,j} = \max \left( x_{i+k,j+l} \right), \quad \forall \, 0 \leq k,l < f
		\end{equation*}
		where \( f \) is the filter size, and \( x \) is the input feature map.
		
		\item \textbf{Average Pooling:} Average pooling computes the average value within a local patch of the feature map. It smooths the feature map and reduces noise by averaging the values.
		\begin{equation*}
			y_{i,j} = \frac{1}{f^2} \sum_{k=0}^{f-1} \sum_{l=0}^{f-1} x_{i+k,j+l}
		\end{equation*}
	\end{itemize}
	
	\paragraph{Example:}
	Consider a 4x4 feature map that we apply 2x2 max pooling to with a stride of 2. The resulting 2x2 feature map will contain the maximum values from each 2x2 patch of the input feature map. This process reduces the spatial dimensions by a factor of 2 while preserving the most significant features.
	
	In summary, pooling layers play a crucial role in reducing the spatial dimensions of feature maps, enhancing computational efficiency, and promoting translation invariance in convolutional neural networks. The most commonly used pooling methods are max pooling and average pooling, each with its own advantages depending on the application.
	
	\subsubsection*{Fully Connected Layers in CNNs}
	After several convolutional and pooling layers, CNNs typically include fully connected layers. These layers integrate the features learned by the convolutional layers to make the final prediction, and perform duties similar to layers in standard NNs \cite{osheaIntroductionConvolutionalNeural2015}.
	
	\subsubsection*{AlexNet}
	AlexNet, introduced by Alex Krizhevsky and colleagues in 2012 \cite{krizhevskyImageNetClassificationDeep2017}, significantly advanced the field of deep learning. It has a deeper architecture than LeNet-5, with five convolutional layers, followed by three fully connected layers. AlexNet's success in the ImageNet competition highlighted the potential of deep CNNs for large-scale image classification.
		
\section*{Methods}
	Different methodologies were employed for predicting species and resistances, and these will be described in their respective sections. However, common practices were followed for both methodologies. Specifically, Python (version 3.11) \cite{python} was used as the programming language. All computations were performed on the GenomeDK cluster (\url{genome.au.dk}) at Aarhus University.
	
	\subsection*{Species classification}
	For species classification, the data used in this thesis was sourced directly from the dataset described in Weis et al. (2022) \cite{weis2021driams,weisDirectAntimicrobialResistance2022}. Specifically, the mass spectra were binned into 6000 fixed bins of 3 Da each, ranging from 2,000 Da to 20,000 Da, to create a consistent 6000-dimensional feature vector for each sample . This binning process ensures that each spectrum is uniformly represented, enabling the effective application of machine learning algorithms.
	
	The preprocessing steps involved in creating the binned data include the following:
	
	\begin{itemize}
		\item \textbf{Mass-to-Charge Ratio Partitioning}: The m/z axis is divided into equal-sized bins of 3 Da.
		\item \textbf{Intensity Summation}: The intensity of all measurements falling within each bin is summed to produce the feature vector.
	\end{itemize}
	
	Prior to creating any models, the data was filtered to exclude data points that included the string "MIX!" in their species label, as it was unclear how this label should be interpreted. Additionally, species containing fewer than five samples were excluded to ensure more reliable modeling. The data was then split into training (80\%) and validation (20\%) sets using a stratified split method based on species to ensure that all species were represented in both sets.
	
	All machine learning models for species classification, including Elastic Net, Ridge, Random Forest, and SVM, were implemented using scikit-learn \cite{scikit-learn}.
	
	\subsection*{Resistance classification}
	For resistance classification, the mass spectra underwent several preprocessing steps to ensure the data was suitable for model training and prediction. These preprocessing steps included log-transformation, smoothing, normalization, and rescaling, as detailed below. These steps were originally implemented in R by PhD student Johan Kjeldbjerg Lassen.
	
	\subsubsection*{Preprocessing steps}
	\begin{enumerate}
		\item \textbf{Log-transformation}: The intensity values of the spectra were log-transformed to stabilize variance and make the data more normally distributed:
		\begin{equation*}
			int_{log} = log(int + 1)
		\end{equation*}
		\item \textbf{Smoothing with LOWESS}: A two-step Locally Weighted Scatterplot Smoothing (LOWESS) procedure was applied to the log-transformed intensities:
		\begin{itemize}
			\item \textbf{First LOWESS}: Applied with a fraction (frac) of \( 0.0008 \) and a regularization parameter (\( \lambda \)) of 40 to smooth out the data.
			\item \textbf{Second LOWESS}: Applied with a fraction (frac) of \( 0.3 \) and a regularization parameter (\( \lambda \)) of 40 to further smooth and normalize the data.
		\end{itemize}
		\item \textbf{Rescaling}: The normalized intensities were rescaled to a range of \( [-1, 1] \):
		\begin{equation}
			int_{rescaled} = 2 \left( \frac{int_{normalized} - int_{min}}{int_{max} - int_{min}} \right) - 1
		\end{equation}
		\item \textbf{Mapping to new scale}: The mass-to-charge ratio (m/z) values were mapped to a new scale ranging from 2000 to 20000 with 0.5 Da increments using a final LOWESS smoothing with a fraction (frac) of \( 0.0003 \) and a regularization parameter (\( \lambda \)) of 40.
	\end{enumerate}
	These preprocessing steps ensured that the spectra were uniformly represented and smoothed, reducing noise and enhancing signal clarity. A visualization of these steps is shown in Figure \ref{fig:preprocessing_steps}.
	
	\subsubsection*{Data filtering and splitting}
	In order to standardize and focus the analysis on the most prevalent resistances, I identified the top 25 most frequent resistances from each dataset in the DRIAMS-A collection using a custom Python script. The script filters out entries labeled as 'MIX!' in the species column, counts the occurrences of each resistance phenotype, and selects the most common resistances. Subsequently, the script consolidates these resistances across all datasets, retaining only those resistances in the filtered datasets, thus ensuring that the final datasets include a consistent set of the most frequent resistance phenotypes for analysis.
	
	The filtered datasets were initially divided into training/validation (90\%) and testing (10\%) sets. Due to the multilabel nature of the resistance classification problem, the split was performed using scikit-multilearn \cite{2017arXiv170201460S}, which ensures an even distribution of label combinations across the splits (as demonstrated in \ref{fig:multilabel_split}). The training/validation set was further divided into training (80\%) and validation (20\%) subsets using the same method.
	
	\subsubsection*{Model implementations}
	Ridge and Random Forest classification models were implemented using sci-kit learn \cite{scikit-learn}. Convolutional Neural Network models were implemented using PyTorch \cite{NEURIPS2019_9015}, PyTorch Lightning \cite{falcon2019pytorch}, and Weights and Biases \cite{wandb}.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.9\linewidth]{img/preprocessing_steps_combined.png}
		\caption{Illustration of the preprocessing steps applied to an original MALDI-TOF MS spectrum. The top plot shows the original spectrum. The middle row includes three subplots displaying the intermediate steps: the log-transformed spectrum (left), the first LOWESS smoothing (middle), and the second LOWESS smoothing with normalization (right). The bottom plot presents the final rescaled and smoothed spectrum. This step-by-step preprocessing enhances the data quality by reducing noise and normalizing intensity values, facilitating more effective downstream analysis.}
		\label{fig:preprocessing_steps}
	\end{figure}
	\clearpage


\section*{Results}

\section*{Discussion}

\newpage
\printbibliography

\clearpage
\renewcommand{\thefigure}{Supplementary \arabic{figure}}
\renewcommand{\figurename}{}
\setcounter{figure}{0}

\section*{Supplementary Materials}

\begin{sidewaysfigure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{img/2018_train_val_test_split.png}
	\caption{Distribution of Label Combinations between Train/Validation and Test Sets. The bar plot shows the frequency of some of the label combinations in the training/validation set (blue) and the test set (green). The x-axis represents the different label combinations, while the y-axis indicates the count of each combination in the respective datasets. This visualization demonstrates that the stratified split maintains the distribution of label combinations across the train/validation and test sets, ensuring that the datasets are representative of each other.}
	\label{fig:multilabel_split}
\end{sidewaysfigure}


\end{document}