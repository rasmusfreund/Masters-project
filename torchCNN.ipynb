{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486264f9-8ed4-43ab-817e-d21ec331f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9009678-47ef-4999-a9dc-a6cfedc07876",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"torchCNN\", entity=\"rasmusfreund\")\n",
    "WANDB_NOTEBOOK_NAME = \"torchCNN.ipynb\"\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.001\n",
    "config.num_epochs = 10\n",
    "config.batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80bf01b3-67ac-44f5-9e2f-7e96afc63884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, csv_file_path: str | list[str], data_dir_path: str | list[str], one_hot_encode: bool = True, training_target: str = 'species', num_workers: int = 5):\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader with paths to the csv file and the directory containing the actual data files.\n",
    "\n",
    "        Parameters:\n",
    "        csv_file_path: File path (or list of paths) to the CSV file(s) containing the references and labels.\n",
    "        data_dir_path: Directory path (or list of paths) that contains the actual data files.\n",
    "        one_hot_encode: True / False statement on whether to one-hot encode species labels\n",
    "        training_target: Sets label output to either 'species' or 'resistance' depending on target of the model\n",
    "        num_workers: Integer defining how many CPU cores are available for the data generator\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Control whether to one-hot encode labels\n",
    "        self.one_hot_encode = one_hot_encode\n",
    "\n",
    "        # Check target of the model\n",
    "        if training_target not in ['species', 'resistance']:\n",
    "            raise ValueError('Argument \"training_target\" should either be \"species\" or \"resistance\".')\n",
    "        else:\n",
    "            self.training_target = training_target\n",
    "        \n",
    "        # Get data references\n",
    "        self.data_references = self._load_data_references(csv_file_path, data_dir_path)\n",
    "        \n",
    "        # Adjust file paths in data_references to include the correct directory\n",
    "        self.data_references['file_path'] = self.data_references.apply(self.assign_file_path, axis=1)\n",
    "        \n",
    "        # Mapping for one-hot encoding of label-vectorx\n",
    "        # Case: species\n",
    "        if self.training_target == 'species':\n",
    "            self.label_mapping = {label: idx for idx, label in enumerate(self.data_references['species'].unique())}\n",
    "        # Case: resistance\n",
    "        else:\n",
    "            self.antibiotic_columns = [col for col in self.data_references.columns[3:] if '_' not in col]\n",
    "            self.label_mapping = {antibiotic: idx for idx, antibiotic in enumerate(self.antibiotic_columns)}\n",
    "        \n",
    "        self.num_labels = len(self.label_mapping)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_references)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.load_data(idx)\n",
    "        return torch.tensor(data, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    def _load_data_references(self, csv_file_path, data_dir_path):\n",
    "        \n",
    "        data_frames = []\n",
    "        if isinstance(csv_file_path, list):\n",
    "            for path, dir_path in zip(csv_file_path, data_dir_path):\n",
    "                df = pd.read_csv(path, low_memory=False)\n",
    "                df['source_csv'] = path\n",
    "                df['data_dir'] = dir_path\n",
    "                data_frames.append(df)\n",
    "        else:\n",
    "            df = pd.read_csv(csv_file_path, low_memory=False)\n",
    "            df['source_csv'] = csv_file_path\n",
    "            df['data_dir'] = data_dir_path\n",
    "            data_frames.append(df)\n",
    "        \n",
    "        return pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    \n",
    "    def assign_file_path(self, row):\n",
    "        \"\"\"\n",
    "        Assigns the correct file path based on the code and directory paths.\n",
    "        \"\"\"\n",
    "        code = row['code']\n",
    "        data_dir = row['data_dir']\n",
    "        file_path = f'{data_dir}/{code}.txt'\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f'File {code}.txt not found in provided directories.')\n",
    "        return file_path\n",
    "\n",
    "    \n",
    "    def load_data(self, index: int):\n",
    "        \"\"\"\n",
    "        Loads a single data file based on the index provided and returns the contents and label.\n",
    "        If target of the model is resistances, (R)esistant and (I)ntermediate will both be marked by 1, indicating resistance.\n",
    "\n",
    "        Parameters:\n",
    "        index: The index of the data file to load, as referenced in the CSV file.\n",
    "\n",
    "        Returns:\n",
    "        data, label pair         \n",
    "        \"\"\"\n",
    "\n",
    "        file_path = self.data_references.iloc[index]['file_path']\n",
    "        #print(f'Loading data from {file_path}')\n",
    "        data = self.convert_file_to_floats(file_path)\n",
    "        if not data:\n",
    "            print(f'No loaded data from: {file_path}')\n",
    "        label = None\n",
    "        \n",
    "        if self.training_target == 'resistance':\n",
    "            # Initialise k-hot vector for resistance data\n",
    "            resistance_vector = [0] * self.num_labels\n",
    "            for col in self.antibiotic_columns:\n",
    "                # Get resistance label for each column\n",
    "                resistance = self.data_references.iloc[index][col]\n",
    "                if resistance in ['R', 'I']: \n",
    "                    col_idx = self.label_mapping[col]\n",
    "                    resistance_vector[col_idx] = 1\n",
    "            label = resistance_vector\n",
    "        else:        \n",
    "            species = self.data_references.iloc[index]['species']\n",
    "            label_index = self.label_mapping[species]\n",
    "            label = np.zeros(self.num_labels)\n",
    "            label[label_index] = 1 if self.one_hot_encode else label_index\n",
    "            \n",
    "        return data, label\n",
    "\n",
    "\n",
    "    def convert_file_to_floats(self, file_path: str):\n",
    "        \n",
    "        data = [] # Container for converted data\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                next(file) # Skip the header\n",
    "                for line in file:\n",
    "                    string_values = line.strip().split(',')[0]\n",
    "                    data.append(float(string_values))\n",
    "        except Exception as e:\n",
    "            print(f'Error processing file {file_path}: {e}')\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def one_hot_encode_label(self, label: str):\n",
    "        \n",
    "        one_hot_vector = np.zeros(self.num_labels)\n",
    "        label_index = self.label_mapping[label]\n",
    "        one_hot_vector[label_index] = 1\n",
    "        \n",
    "        return one_hot_vector\n",
    "\n",
    "\n",
    "    def get_resistances_from_vector(self, k_hot_vector):\n",
    "\n",
    "        index_to_antibiotic = {v: k for k, v in self.label_mapping.items()}\n",
    "        active_indeces = [i for i, val in enumerate(k_hot_vector) if val == 1]\n",
    "        active_resistances = [index_to_antibiotic[i] for i in active_indeces]\n",
    "\n",
    "        return active_resistances\n",
    "    \n",
    "\n",
    "    def data_generator(self):\n",
    "        \"\"\"\n",
    "        Generator function that yields one data-label pair at a time; parallelised.\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "            future_to_idx = {executor.submit(self.load_data, idx): idx for idx in range(len(self.data_references))}\n",
    "            for future in as_completed(future_to_idx):\n",
    "                yield future.result()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4d750-3516-4ade-8c01-4c733c459b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863fa52-ff78-476c-bead-cf199f62682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRidge(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(TorchRidge, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.reduce_conv1 = nn.Conv1d(16, 16, kernel_size=16, stride=16)\n",
    "        self.fc = nn.Linear(2240, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.reduce_conv1(x)\n",
    "        x = self.reduce_conv1(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten tensor\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f3f95-3bff-4198-b694-3a910963986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_accuracy(outputs, labels):\n",
    "    predictions = outputs > 0.5\n",
    "    correct_pred = (predictions == labels).float()\n",
    "    accuracy = correct_pred.sum() / (len(predictions) * predictions.size(1))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96578d17-f2b4-4830-b375-b2dc309c1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += multilabel_accuracy(outputs, targets)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_accuracy = total_accuracy / len(train_loader)\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, targets = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += multilabel_accuracy(outputs, targets)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_accuracy = total_accuracy / len(test_loader)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354315cd-cd7e-47ce-b3c1-356c08701f4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(csv_file_path='/faststorage/project/amr_driams/rasmus/data/DRIAMS-A/id/2015_clean_train_val.csv',\n",
    "                         data_dir_path='/faststorage/project/amr_driams/data/DRIAMS-A/preprocessed_raw/2015',\n",
    "                         training_target='resistance',     \n",
    "                         num_workers=4)\n",
    "total_data_points = len(data_loader)\n",
    "\n",
    "X, y = [], []\n",
    "for data, label in tqdm(data_loader.data_generator(), total=total_data_points, desc=\"Loading Data\"):\n",
    "    X.append(data)\n",
    "    y.append(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37af61b-cbca-4580-a0a5-ca35dc98d733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File 9681cb39-ace9-47f6-a93f-3d3a5e0c9b2d.txt not found in provided directories.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/faststorage/project/amr_driams/rasmus/data/DRIAMS-A/id/2016_clean_train_val.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdata_dir_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/faststorage/project/amr_driams/data/DRIAMS-A/preprocessed_raw/2016\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtraining_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresistance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m total_data_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n\u001b[1;32m      7\u001b[0m X, y \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, csv_file_path, data_dir_path, one_hot_encode, training_target, num_workers)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_references \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_data_references(csv_file_path, data_dir_path)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Adjust file paths in data_references to include the correct directory\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_references[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_references\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Mapping for one-hot encoding of label-vectorx\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Case: species\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/amr/lib/python3.11/site-packages/pandas/core/frame.py:10347\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10333\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10335\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10336\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10337\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10345\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10346\u001b[0m )\n\u001b[0;32m> 10347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/amr/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/amr/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/miniforge3/envs/amr/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36mDataLoader.assign_file_path\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m     75\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt not found in provided directories.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File 9681cb39-ace9-47f6-a93f-3d3a5e0c9b2d.txt not found in provided directories."
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(csv_file_path='/faststorage/project/amr_driams/rasmus/data/DRIAMS-A/id/2016_clean_train_val.csv',\n",
    "                         data_dir_path='/faststorage/project/amr_driams/data/DRIAMS-A/preprocessed_raw/2016',\n",
    "                         training_target='resistance',     \n",
    "                         num_workers=5)\n",
    "total_data_points = len(data_loader)\n",
    "\n",
    "X, y = [], []\n",
    "for _, label in tqdm(data_loader.data_generator(), total=total_data_points, desc=\"Loading Data\"):\n",
    "    y.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c27143-4ad3-4495-bbf7-dcc84b9dd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(y[0])\n",
    "naive_guess = [0 for i in range(num_classes)]\n",
    "accuracies = []\n",
    "for ground_truth in y:\n",
    "    accuracy = sum([1 for gt, ng in zip(ground_truth, naive_guess) if gt == ng]) / num_classes\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "mean_acc = sum(accuracies) / len(accuracies)\n",
    "print(\"Mean accuracy for naive guess:\", mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81cc1d-6f66-4e0e-9c5f-264e99fc61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TorchData(X_train, y_train)\n",
    "test_dataset = TorchData(X_test, y_test)\n",
    "\n",
    "train_loader = TorchDataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "test_loader = TorchDataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b6cb0-d0a5-4d8b-91c9-82af2803e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TorchRidge(num_labels=len(y[0]))\n",
    "optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "device = 'cpu'\n",
    "\n",
    "# Training loop\n",
    "train_loss_hist, train_accuracy_hist = [], []\n",
    "test_loss_hist, test_accuracy_hist = [], []\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f'Initialising epoch {epoch+1}/{config.num_epochs}')\n",
    "\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_accuracy = validate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}\\nLoss: {train_loss:.4f}\\nAccuracy: {train_accuracy:.4f}\\nTest Loss: {test_loss:.4f}\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    train_accuracy_hist.append(train_accuracy)\n",
    "    test_loss_hist.append(test_loss)\n",
    "    test_accuracy_hist.append(test_accuracy)\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy\n",
    "    })\n",
    "    wandb.save('model_path')\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
